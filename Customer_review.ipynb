{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This part of code will skip all the un-necessary warnings which can occur during the execution of this project.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=Warning)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation for GPU llama-cpp-python==0.2.28\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69\n",
        "!pip install -q --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft==0.12.0 accelerate==0.32.1 bitsandbytes==0.43.2\n",
        "!pip install rich\n",
        "!pip install tqdm\n",
        "# Installation for the huggingface-hub\n",
        "!pip install huggingface-hub\n",
        "# installation for the datasets\n",
        "!pip install datasets\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Collecting llama-cpp-python==0.2.69\n",
        "  Downloading llama_cpp_python-0.2.69.tar.gz (42.5 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 42.5/42.5 MB 41.6 MB/s eta 0:00:0000:0100:01\n",
        "  Installing build dependencies ... done\n",
        "  Getting requirements to build wheel ... done\n",
        "  Installing backend dependencies ... done\n",
        "  Preparing metadata (pyproject.toml) ... done\n",
        "Collecting diskcache>=5.6.1\n",
        "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 45.5/45.5 KB 17.6 MB/s eta 0:00:00\n",
        "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.69) (3.1.4)\n",
        "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.69) (4.12.2)\n",
        "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.69) (1.26.4)\n",
        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.69) (2.1.5)\n",
        "Building wheels for collected packages: llama-cpp-python\n",
        "  Building wheel for llama-cpp-python (pyproject.toml) ... -"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install numpy\n",
        "!pip install numpy\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
        "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#  Import hf_hub_download from the huggingface_hub\n",
        "#  Import Llama from the llama_cpp library\n",
        "# Import load_dataset from datasets\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#  model name should be the \"Llama-13B-chat-GGUF\" from \"TheBloke\"\n",
        "# model basename should be the Quantized model we want to use here - This should be the \"Q5_K_M.gguf\"\n",
        "model_name = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "model_basename = \"llama-2-13b-chat.Q5_K_M.gguf\" # the model is in gguf format\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(\n",
        "    repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\", # mention the variable \"model_name\" here\n",
        "    filename=\"llama-2-13b-chat.Q5_K_M.gguf\" # mention the variable \"model_basename\" here\n",
        "    )\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# del lcpp_llm\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "lcpp_llm = Llama(\n",
        "        model_path=model_path,\n",
        "        n_threads=2,  # CPU cores\n",
        "        n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "        n_gpu_layers=43,  # Change this value based on your model and your GPU VRAM pool.\n",
        "        n_ctx=4096,  # Context window\n",
        "    )\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /root/.cache/huggingface/hub/models--TheBloke--Llama-2-13B-chat-GGUF/snapshots/4458acc949de0a9914c3eab623904d4fe999050a/llama-2-13b-chat.Q5_K_M.gguf (version GGUF V2)\n",
        "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
        "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
        "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
        "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
        "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
        "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
        "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
        "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
        "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
        "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
        "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
        "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
        "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
        "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
        "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
        "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
        "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
        "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
        "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
        "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
        "llama_model_loader: - type  f32:   81 tensors\n",
        "llama_model_loader: - type q5_K:  241 tensors\n",
        "llama_model_loader: - type q6_K:   41 tensors\n",
        "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
        "llm_load_print_meta: format           = GGUF V2\n",
        "llm_load_print_meta: arch             = llama\n",
        "llm_load_print_meta: vocab type       = SPM\n",
        "llm_load_print_meta: n_vocab          = 32000\n",
        "llm_load_print_meta: n_merges         = 0\n",
        "llm_load_print_meta: n_ctx_train      = 4096\n",
        "llm_load_print_meta: n_embd           = 5120\n",
        "llm_load_print_meta: n_head           = 40\n",
        "llm_load_print_meta: n_head_kv        = 40\n",
        "llm_load_print_meta: n_layer          = 40\n",
        "llm_load_print_meta: n_rot            = 128\n",
        "llm_load_print_meta: n_embd_head_k    = 128\n",
        "llm_load_print_meta: n_embd_head_v    = 128\n",
        "llm_load_print_meta: n_gqa            = 1\n",
        "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
        "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
        "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
        "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
        "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
        "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
        "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
        "llm_load_print_meta: n_ff             = 13824\n",
        "llm_load_print_meta: n_expert         = 0\n",
        "llm_load_print_meta: n_expert_used    = 0\n",
        "llm_load_print_meta: causal attn      = 1\n",
        "llm_load_print_meta: pooling type     = 0\n",
        "llm_load_print_meta: rope type        = 0\n",
        "llm_load_print_meta: rope scaling     = linear\n",
        "llm_load_print_meta: freq_base_train  = 10000.0\n",
        "llm_load_print_meta: freq_scale_train = 1\n",
        "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
        "llm_load_print_meta: rope_finetuned   = unknown\n",
        "llm_load_print_meta: ssm_d_conv       = 0\n",
        "llm_load_print_meta: ssm_d_inner      = 0\n",
        "llm_load_print_meta: ssm_d_state      = 0\n",
        "llm_load_print_meta: ssm_dt_rank      = 0\n",
        "llm_load_print_meta: model type       = 13B\n",
        "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
        "llm_load_print_meta: model params     = 13.02 B\n",
        "llm_load_print_meta: model size       = 8.60 GiB (5.67 BPW) \n",
        "llm_load_print_meta: general.name     = LLaMA v2\n",
        "llm_load_print_meta: BOS token        = 1 '<s>'\n",
        "llm_load_print_meta: EOS token        = 2 '</s>'\n",
        "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
        "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
        "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
        "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
        "ggml_cuda_init: found 1 CUDA devices:\n",
        "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
        "llm_load_tensors: ggml ctx size =    0.37 MiB\n",
        "llm_load_tensors: offloading 40 repeating layers to GPU\n",
        "llm_load_tensors: offloading non-repeating layers to GPU\n",
        "llm_load_tensors: offloaded 41/41 layers to GPU\n",
        "llm_load_tensors:        CPU buffer size =   107.42 MiB\n",
        "llm_load_tensors:      CUDA0 buffer size =  8694.21 MiB\n",
        "....................................................................................................\n",
        "llama_new_context_with_model: n_ctx      = 4096\n",
        "llama_new_context_with_model: n_batch    = 512\n",
        "llama_new_context_with_model: n_ubatch   = 512\n",
        "llama_new_context_with_model: flash_attn = 0\n",
        "llama_new_context_with_model: freq_base  = 10000.0\n",
        "llama_new_context_with_model: freq_scale = 1\n",
        "llama_kv_cache_init:      CUDA0 KV buffer size =  3200.00 MiB\n",
        "llama_new_context_with_model: KV self size  = 3200.00 MiB, K (f16): 1600.00 MiB, V (f16): 1600.00 MiB\n",
        "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
        "llama_new_context_with_model:      CUDA0 compute buffer size =   368.00 MiB\n",
        "llama_new_context_with_model:  CUDA_Host compute buffer size =    18.01 MiB\n",
        "llama_new_context_with_model: graph nodes  = 1286\n",
        "llama_new_context_with_model: graph splits = 2\n",
        "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
        "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n",
        "Using fallback chat format: None\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews_df = pd.read_csv('customer_reviews_dataset.csv')\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews_df\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a \"Laptop\" reviews DaatFrame based on the \"product_type\" column in the dataset\n",
        "laptop_reviews = sample_reviews_df[sample_reviews_df['product_type'] == 'Laptop']\n",
        "\n",
        "# Create a \"Headphones\" reviews DaatFrame based on the \"product_type\" column in the dataset\n",
        "headphones_reviews = sample_reviews_df[sample_reviews_df['product_type'] == 'Headphones']\n",
        "\n",
        "# Create a \"Smartphone\" reviews DaatFrame based on the \"product_type\" column in the dataset\n",
        "smartphone_reviews = sample_reviews_df[sample_reviews_df['product_type'] == 'Smartphone']\n",
        "\n",
        "# Create a \"Power Bank\" reviews DaatFrame based on the \"product_type\" column in the dataset\n",
        "power_bank_reviews = sample_reviews_df[sample_reviews_df['product_type'] == 'Power Bank']\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "laptop_gold_examples = laptop_reviews.sample(2, random_state=40)\n",
        "headphones_gold_examples = headphones_reviews.sample(2, random_state=40)\n",
        "smartphone_gold_examples = smartphone_reviews.sample(2, random_state=40)\n",
        "power_bank_gold_examples = power_bank_reviews.sample(2, random_state=40)\n",
        "\n",
        "# Concatenate positive and negative gold examples\n",
        "sample_reviews_gold_examples_df = pd.concat([laptop_gold_examples, headphones_gold_examples,smartphone_gold_examples,power_bank_gold_examples])\n",
        "\n",
        "# Create the training set by excluding gold examples\n",
        "sample_reviews_examples_df = sample_reviews_df.drop(index=sample_reviews_gold_examples_df.index)\n",
        "\n",
        "# Convert gold examples to JSON\n",
        "columns_to_select = ['review_text', 'product_type','aspects_review']\n",
        "gold_examples_json = sample_reviews_gold_examples_df[columns_to_select].to_json(orient='records')\n",
        "\n",
        "# Print the first record from the JSON\n",
        "print(json.loads(gold_examples_json)[0])\n",
        "\n",
        "# Print the shapes of the datasets\n",
        "print(\"Training Set Shape:\", sample_reviews_examples_df.shape)\n",
        "print(\"Gold Examples Shape:\", sample_reviews_gold_examples_df.shape)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "{'review_text': \"Originally bought it for my work, quite happy with it so far! Fast, reliable, easy to use and has a good webcam. Display is good and battery backup is also great. The keyboard is a joy to type on, gives me the old typewriter vibes! Quickly become my main laptop for everyday use, and I'm very satisfied with my purchase.\", 'product_type': 'Laptop', 'aspects_review': '{ battery : Positive , keyboard : Positive , display : Positive }'}\n",
        "Training Set Shape: (22, 11)\n",
        "Gold Examples Shape: (8, 11)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gold_examples = (\n",
        "        sample_reviews_gold_examples_df.loc[:, columns_to_select]\n",
        "                                     .sample(8, random_state=40) #<- ensures that gold examples are the same for every session\n",
        "                                     .to_json(orient='records')\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_system_message =  \"\"\"\n",
        "### Example 1\n",
        "{\n",
        "    'review_text': \"These headphones are amazing! The sound quality is excellent, and the comfort is top-notch.\",\n",
        "    'sentiment': 'Positive',\n",
        "    'product_type': 'Headphones'\n",
        "}\n",
        "\n",
        "### Example 2\n",
        "{\n",
        "    'review_text': \"The battery life on this smartphone is really disappointing. It barely lasts a day.\",\n",
        "    'sentiment': 'Negative',\n",
        "    'product_type': 'Smartphone'\n",
        "}\n",
        "\n",
        "### Example 3\n",
        "{\n",
        "    'review_text': \"This laptop is great for the price. It performs well for everyday tasks, but the screen resolution could be better.\",\n",
        "    'sentiment': 'Neutral',\n",
        "    'product_type': 'Laptop'\n",
        "}\n",
        "\"\"\"\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_llama_response(system_message, few_shot_examples, new_review, temp):\n",
        "    # Combine user_prompt and system_message to create the prompt using f-string\n",
        "    prompt = f\"[INST]{{{system_message}}}\\n{{{few_shot_examples}}}\\n'user': ```{user_message_template.format(review=new_review)}```[INST]\"\n",
        "\n",
        "    # Generate a response from the LLM model\n",
        "    response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        max_tokens=256,\n",
        "        temperature=temp,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        top_k=50,\n",
        "        stop=['INST'],\n",
        "        echo=False\n",
        "    )\n",
        "\n",
        "    # Extract and return the response text\n",
        "    response_text = response[\"choices\"][0][\"text\"]\n",
        "    return response_text\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_examples_with_seed(dataset, n=2, random_seed=None):\n",
        "    \"\"\"\n",
        "    Return two DataFrames with randomized examples of size 2n with two classes.\n",
        "    Create subsets of each class, choose random samples from the subsets,\n",
        "    merge and randomize the order of samples in the merged list.\n",
        "    Each run of this function creates a different random sample of examples\n",
        "    chosen from the training data.\n",
        "\n",
        "    Args:\n",
        "        dataset (DataFrame): A DataFrame with examples (text + label)\n",
        "        n (int): number of examples of each class to be selected\n",
        "        random_seed (int): seed for reproducibility (default is None)\n",
        "\n",
        "    Output:\n",
        "        few_shot_examples_df (DataFrame): A DataFrame with examples in random order\n",
        "        new_df (DataFrame): A new DataFrame excluding selected examples\n",
        "    \"\"\"\n",
        "\n",
        "    laptop_reviews = (dataset.product_type == 'Laptop')\n",
        "    headphone_reviews = (dataset.product_type == 'Headphones')\n",
        "    power_bank_reviews = (dataset.product_type == 'Power Bank')\n",
        "    smartphone_reviews = (dataset.product_type == 'Smartphone')\n",
        "    columns_to_select = ['review_text', 'product_type','aspects_review']\n",
        "\n",
        "    # Set a fixed random seed for reproducibility\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    laptop_examples = dataset.loc[laptop_reviews, columns_to_select].sample(n)\n",
        "    headphone_examples = dataset.loc[headphone_reviews, columns_to_select].sample(n)\n",
        "    power_bank_examples = dataset.loc[power_bank_reviews, columns_to_select].sample(n)\n",
        "    smartphone_examples = dataset.loc[smartphone_reviews, columns_to_select].sample(n)\n",
        "\n",
        "    few_shot_examples_df = pd.concat([laptop_examples, headphone_examples, power_bank_examples, smartphone_examples])\n",
        "\n",
        "    # sampling without replacement is equivalent to random shuffling\n",
        "    few_shot_examples_df = few_shot_examples_df.sample( 4*n, replace=False)\n",
        "\n",
        "    # Create a new DataFrame excluding selected examples\n",
        "    new_df = dataset.drop(index=few_shot_examples_df.index)\n",
        "\n",
        "    return few_shot_examples_df, new_df\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_combined_aspect_and_product_accuracy(df):\n",
        "    correct_predictions_count = 0\n",
        "\n",
        "    # Function to parse aspect-based sentiment string into a dictionary\n",
        "    def parse_aspects(aspect_string):\n",
        "        aspect_string= str(aspect_string)\n",
        "        aspect_string = re.sub(r'[{}]', '', aspect_string)  # Remove curly braces\n",
        "        aspects = re.split(r',\\s*', aspect_string)  # Split into individual aspects\n",
        "\n",
        "        try:\n",
        "         dict_res = dict(re.split(r'\\s*:\\s*', aspect) for aspect in aspects if aspect)\n",
        "        except:\n",
        "          dict_res={}\n",
        "\n",
        "        return dict_res\n",
        "\n",
        "\n",
        "    # Function to normalize text by removing spaces and converting to lowercase\n",
        "    def normalize_text(text):\n",
        "        return ''.join(text.split()).lower() if text is not None else None\n",
        "\n",
        "    # Iterate over each row to compare product type and aspect-based sentiments\n",
        "    for index, row in df.iterrows():\n",
        "        predicted_product_type = normalize_text(row['predicted_product_type'])\n",
        "        actual_product_type = normalize_text(row['product_type'])\n",
        "        predicted_aspects = parse_aspects(row['predicted_aspect_based_sentiment'])\n",
        "        actual_aspects = parse_aspects(row['aspects_review'])\n",
        "\n",
        "        # Check if product type matches and all aspects match in sentiment\n",
        "        if predicted_product_type == actual_product_type and \\\n",
        "          predicted_aspects != '' and \\\n",
        "          all(predicted_aspects.get(key, '').lower() == value.lower() for key, value in actual_aspects.items()):\n",
        "            correct_predictions_count += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (correct_predictions_count / len(df)) * 100\n",
        "    return accuracy\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "user_message_template = \"{review}\"\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_product_type(text):\n",
        "    text=str(text)\n",
        "    match = re.search(r'\\[([^:]+)', text)\n",
        "    return match.group(1).strip() if match else None\n",
        "\n",
        "def extract_aspect_based_sentiment(text):\n",
        "    text=str(text)\n",
        "    match = re.search(r'\\{([^}]+)\\}', text)\n",
        "    return \"{ \" + match.group(1).strip() + \" }\" if match else None\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list = []\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews_examples_df\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    few_shot_examples_df ,sample_reviews_df = create_examples_with_seed(sample_reviews_examples_df, n=1 , random_seed = i)\n",
        "\n",
        "    review_example1 = few_shot_examples_df.iloc[0].review_text\n",
        "    review_example2 = few_shot_examples_df.iloc[1].review_text\n",
        "    review_example3 = few_shot_examples_df.iloc[2].review_text\n",
        "    review_example4 = few_shot_examples_df.iloc[3].review_text\n",
        "\n",
        "    assistant_output_example1 = \"[ \" + few_shot_examples_df.iloc[0].product_type.lower() + \" : \" + few_shot_examples_df.iloc[0].aspects_review.lower() + \" ]\"\n",
        "    assistant_output_example2 = \"[ \" + few_shot_examples_df.iloc[1].product_type.lower() + \" : \" + few_shot_examples_df.iloc[1].aspects_review.lower() + \" ]\"\n",
        "    assistant_output_example3 = \"[ \" + few_shot_examples_df.iloc[2].product_type.lower() + \" : \" + few_shot_examples_df.iloc[2].aspects_review.lower() + \" ]\"\n",
        "    assistant_output_example4 = \"[ \" + few_shot_examples_df.iloc[3].product_type.lower() + \" : \" + few_shot_examples_df.iloc[3].aspects_review.lower() + \" ]\"\n",
        "\n",
        "    few_shot_examples = [\n",
        "        {'role':'user', 'content': user_message_template.format(review=review_example1)},\n",
        "        {'role':'assistant', 'content': f\"{assistant_output_example1}\"},\n",
        "        {'role':'user', 'content': user_message_template.format(review=review_example2)},\n",
        "        {'role':'assistant', 'content': f\"{assistant_output_example2}\"},\n",
        "        {'role':'user', 'content': user_message_template.format(review=review_example3)},\n",
        "        {'role':'assistant', 'content': f\"{assistant_output_example3}\"},\n",
        "        {'role':'user', 'content': user_message_template.format(review=review_example4)},\n",
        "        {'role':'assistant', 'content': f\"{assistant_output_example4}\"}\n",
        "                        ]\n",
        "\n",
        "    few_shot_examples_str = json.dumps(few_shot_examples)\n",
        "\n",
        "\n",
        "    sample_reviews = sample_reviews_df.review_text.values\n",
        "    sentiment_predictions = []\n",
        "    # generate_llama_response( few_shot_system_message , few_shot_examples_str  , input_text , 0.1 )\n",
        "    for sample_review in tqdm(sample_reviews):\n",
        "        try:\n",
        "            #predict sentiments (2 Marks)\n",
        "            sentiment_predictions.append(generate_llama_response( few_shot_system_message , few_shot_examples_str  , sample_review , 0.1 ))\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            sentiment_predictions.append(\"\")\n",
        "    sentiment_predictions\n",
        "    sample_reviews_df[\"sentiment_prediction\"] = sentiment_predictions\n",
        "\n",
        "    pattern = r'\\[\\s*.*?\\s*:\\s*\\{.*?\\}\\s*\\]'\n",
        "\n",
        "    # Function to extract the sentiment data\n",
        "    def extract_sentiment(text):\n",
        "        match = re.findall(pattern, text)\n",
        "        return match[0] if match else None\n",
        "\n",
        "\n",
        "    # Apply the function to each row in the DataFrame\n",
        "    sample_reviews_df['extracted_sentiment'] = sample_reviews_df['sentiment_prediction'].apply(extract_sentiment)\n",
        "\n",
        "    sample_reviews_df\n",
        "    # Function to extract product type\n",
        "    print(sample_reviews_df['extracted_sentiment'])\n",
        "\n",
        "    # Apply the functions to each row in the DataFrame\n",
        "    try:\n",
        "          sample_reviews_df['predicted_product_type'] = sample_reviews_df['extracted_sentiment'].apply(extract_product_type)\n",
        "          sample_reviews_df['predicted_aspect_based_sentiment'] = sample_reviews_df['extracted_sentiment'].apply(extract_aspect_based_sentiment)\n",
        "    except Exception as e:\n",
        "            print(e)\n",
        "            print(sample_reviews_df['predicted_product_type'] )\n",
        "\n",
        "    sample_reviews_df.predicted_aspect_based_sentiment.value_counts()\n",
        "\n",
        "\n",
        "\n",
        "    # Compute Combimed Accuracy (1 Mark) - Call the \"compute_combined_aspect_and_product_accuracy\" with \"sample_reviews_df\" as input datafram\n",
        "    combined_accuracy = compute_combined_aspect_and_product_accuracy(sample_reviews_df)\n",
        "\n",
        "    print(f\"Combined Product Type and Aspect-Based Sentiment Accuracy: {combined_accuracy}%\")\n",
        "    res = combined_accuracy\n",
        "    # Append the \"res\" to the \"accuracy_list\" (1 Mark)\n",
        "    accuracy_list.append(res)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "  0%|          | 0/18 [00:00<?, ?it/s]\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     130.83 ms /   250 runs   (    0.52 ms per token,  1910.94 tokens per second)\n",
        "llama_print_timings: prompt eval time =    1742.66 ms /   941 tokens (    1.85 ms per token,   539.98 tokens per second)\n",
        "llama_print_timings:        eval time =   12637.15 ms /   249 runs   (   50.75 ms per token,    19.70 tokens per second)\n",
        "llama_print_timings:       total time =   15213.59 ms /  1190 tokens\n",
        "  6%|\u258c         | 1/18 [00:15<04:18, 15.22s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     136.61 ms /   256 runs   (    0.53 ms per token,  1874.00 tokens per second)\n",
        "llama_print_timings: prompt eval time =     382.68 ms /   101 tokens (    3.79 ms per token,   263.93 tokens per second)\n",
        "llama_print_timings:        eval time =   13409.47 ms /   255 runs   (   52.59 ms per token,    19.02 tokens per second)\n",
        "llama_print_timings:       total time =   14657.47 ms /   356 tokens\n",
        " 11%|\u2588         | 2/18 [00:29<03:58, 14.89s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     138.39 ms /   256 runs   (    0.54 ms per token,  1849.90 tokens per second)\n",
        "llama_print_timings: prompt eval time =     378.22 ms /    92 tokens (    4.11 ms per token,   243.24 tokens per second)\n",
        "llama_print_timings:        eval time =   13269.14 ms /   255 runs   (   52.04 ms per token,    19.22 tokens per second)\n",
        "llama_print_timings:       total time =   14517.37 ms /   347 tokens\n",
        " 17%|\u2588\u258b        | 3/18 [00:44<03:40, 14.73s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     133.42 ms /   256 runs   (    0.52 ms per token,  1918.72 tokens per second)\n",
        "llama_print_timings: prompt eval time =     506.34 ms /   175 tokens (    2.89 ms per token,   345.62 tokens per second)\n",
        "llama_print_timings:        eval time =   13607.03 ms /   255 runs   (   53.36 ms per token,    18.74 tokens per second)\n",
        "llama_print_timings:       total time =   14981.01 ms /   430 tokens\n",
        " 22%|\u2588\u2588\u258f       | 4/18 [00:59<03:27, 14.83s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     137.05 ms /   256 runs   (    0.54 ms per token,  1867.97 tokens per second)\n",
        "llama_print_timings: prompt eval time =     386.49 ms /    99 tokens (    3.90 ms per token,   256.15 tokens per second)\n",
        "llama_print_timings:        eval time =   13789.36 ms /   255 runs   (   54.08 ms per token,    18.49 tokens per second)\n",
        "llama_print_timings:       total time =   15061.14 ms /   354 tokens\n",
        " 28%|\u2588\u2588\u258a       | 5/18 [01:14<03:13, 14.92s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     129.82 ms /   244 runs   (    0.53 ms per token,  1879.45 tokens per second)\n",
        "llama_print_timings: prompt eval time =     401.77 ms /   123 tokens (    3.27 ms per token,   306.14 tokens per second)\n",
        "llama_print_timings:        eval time =   13477.49 ms /   243 runs   (   55.46 ms per token,    18.03 tokens per second)\n",
        "llama_print_timings:       total time =   14720.35 ms /   366 tokens\n",
        " 33%|\u2588\u2588\u2588\u258e      | 6/18 [01:29<02:58, 14.85s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      39.05 ms /    74 runs   (    0.53 ms per token,  1894.96 tokens per second)\n",
        "llama_print_timings: prompt eval time =     472.94 ms /   134 tokens (    3.53 ms per token,   283.33 tokens per second)\n",
        "llama_print_timings:        eval time =    4069.80 ms /    73 runs   (   55.75 ms per token,    17.94 tokens per second)\n",
        "llama_print_timings:       total time =    4789.22 ms /   207 tokens\n",
        " 39%|\u2588\u2588\u2588\u2589      | 7/18 [01:33<02:07, 11.56s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     113.75 ms /   226 runs   (    0.50 ms per token,  1986.87 tokens per second)\n",
        "llama_print_timings: prompt eval time =     478.02 ms /   131 tokens (    3.65 ms per token,   274.05 tokens per second)\n",
        "llama_print_timings:        eval time =   12721.73 ms /   225 runs   (   56.54 ms per token,    17.69 tokens per second)\n",
        "llama_print_timings:       total time =   13966.56 ms /   356 tokens\n",
        " 44%|\u2588\u2588\u2588\u2588\u258d     | 8/18 [01:47<02:03, 12.33s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     138.50 ms /   256 runs   (    0.54 ms per token,  1848.42 tokens per second)\n",
        "llama_print_timings: prompt eval time =     370.21 ms /    70 tokens (    5.29 ms per token,   189.08 tokens per second)\n",
        "llama_print_timings:        eval time =   14330.15 ms /   255 runs   (   56.20 ms per token,    17.79 tokens per second)\n",
        "llama_print_timings:       total time =   15596.28 ms /   325 tokens\n",
        " 50%|\u2588\u2588\u2588\u2588\u2588     | 9/18 [02:03<02:00, 13.35s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     129.34 ms /   256 runs   (    0.51 ms per token,  1979.29 tokens per second)\n",
        "llama_print_timings: prompt eval time =     371.71 ms /    73 tokens (    5.09 ms per token,   196.39 tokens per second)\n",
        "llama_print_timings:        eval time =   14419.50 ms /   255 runs   (   56.55 ms per token,    17.68 tokens per second)\n",
        "llama_print_timings:       total time =   15679.39 ms /   328 tokens\n",
        " 56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 10/18 [02:19<01:52, 14.07s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      47.31 ms /    86 runs   (    0.55 ms per token,  1817.76 tokens per second)\n",
        "llama_print_timings: prompt eval time =     393.96 ms /   112 tokens (    3.52 ms per token,   284.29 tokens per second)\n",
        "llama_print_timings:        eval time =    4757.38 ms /    85 runs   (   55.97 ms per token,    17.87 tokens per second)\n",
        "llama_print_timings:       total time =    5444.01 ms /   197 tokens\n",
        " 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 11/18 [02:24<01:20, 11.44s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     137.96 ms /   256 runs   (    0.54 ms per token,  1855.58 tokens per second)\n",
        "llama_print_timings: prompt eval time =     394.72 ms /   109 tokens (    3.62 ms per token,   276.14 tokens per second)\n",
        "llama_print_timings:        eval time =   14495.20 ms /   255 runs   (   56.84 ms per token,    17.59 tokens per second)\n",
        "llama_print_timings:       total time =   15798.29 ms /   364 tokens\n",
        " 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 12/18 [02:40<01:16, 12.76s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     137.91 ms /   256 runs   (    0.54 ms per token,  1856.27 tokens per second)\n",
        "llama_print_timings: prompt eval time =     399.67 ms /   121 tokens (    3.30 ms per token,   302.75 tokens per second)\n",
        "llama_print_timings:        eval time =   14501.51 ms /   255 runs   (   56.87 ms per token,    17.58 tokens per second)\n",
        "llama_print_timings:       total time =   15798.76 ms /   376 tokens\n",
        " 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 13/18 [02:56<01:08, 13.69s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     143.10 ms /   256 runs   (    0.56 ms per token,  1788.95 tokens per second)\n",
        "llama_print_timings: prompt eval time =     379.50 ms /    89 tokens (    4.26 ms per token,   234.52 tokens per second)\n",
        "llama_print_timings:        eval time =   14436.36 ms /   255 runs   (   56.61 ms per token,    17.66 tokens per second)\n",
        "llama_print_timings:       total time =   15746.11 ms /   344 tokens\n",
        " 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 14/18 [03:12<00:57, 14.31s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     142.39 ms /   256 runs   (    0.56 ms per token,  1797.85 tokens per second)\n",
        "llama_print_timings: prompt eval time =     488.20 ms /   152 tokens (    3.21 ms per token,   311.35 tokens per second)\n",
        "llama_print_timings:        eval time =   14491.49 ms /   255 runs   (   56.83 ms per token,    17.60 tokens per second)\n",
        "llama_print_timings:       total time =   15927.54 ms /   407 tokens\n",
        " 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 15/18 [03:28<00:44, 14.80s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     135.47 ms /   256 runs   (    0.53 ms per token,  1889.69 tokens per second)\n",
        "llama_print_timings: prompt eval time =     384.66 ms /    94 tokens (    4.09 ms per token,   244.37 tokens per second)\n",
        "llama_print_timings:        eval time =   14491.17 ms /   255 runs   (   56.83 ms per token,    17.60 tokens per second)\n",
        "llama_print_timings:       total time =   15784.07 ms /   349 tokens\n",
        " 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 16/18 [03:43<00:30, 15.10s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     131.12 ms /   256 runs   (    0.51 ms per token,  1952.47 tokens per second)\n",
        "llama_print_timings: prompt eval time =     376.42 ms /    80 tokens (    4.71 ms per token,   212.53 tokens per second)\n",
        "llama_print_timings:        eval time =   14486.52 ms /   255 runs   (   56.81 ms per token,    17.60 tokens per second)\n",
        "llama_print_timings:       total time =   15765.66 ms /   335 tokens\n",
        " 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 17/18 [03:59<00:15, 15.30s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     135.76 ms /   256 runs   (    0.53 ms per token,  1885.72 tokens per second)\n",
        "llama_print_timings: prompt eval time =     371.46 ms /    73 tokens (    5.09 ms per token,   196.52 tokens per second)\n",
        "llama_print_timings:        eval time =   14417.68 ms /   255 runs   (   56.54 ms per token,    17.69 tokens per second)\n",
        "llama_print_timings:       total time =   15696.42 ms /   328 tokens\n",
        "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [04:15<00:00, 14.18s/it]\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "0     None\n",
        "1     None\n",
        "2     None\n",
        "6     None\n",
        "7     None\n",
        "8     None\n",
        "9     None\n",
        "10    None\n",
        "11    None\n",
        "15    None\n",
        "17    None\n",
        "18    None\n",
        "19    None\n",
        "20    None\n",
        "21    None\n",
        "25    None\n",
        "26    None\n",
        "28    None\n",
        "Name: extracted_sentiment, dtype: object\n",
        "Combined Product Type and Aspect-Based Sentiment Accuracy: 0.0%\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "  0%|          | 0/18 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     131.00 ms /   254 runs   (    0.52 ms per token,  1938.92 tokens per second)\n",
        "llama_print_timings: prompt eval time =    1407.63 ms /   711 tokens (    1.98 ms per token,   505.10 tokens per second)\n",
        "llama_print_timings:        eval time =   14386.86 ms /   253 runs   (   56.87 ms per token,    17.59 tokens per second)\n",
        "llama_print_timings:       total time =   16677.26 ms /   964 tokens\n",
        "  6%|\u258c         | 1/18 [00:16<04:43, 16.68s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     131.82 ms /   256 runs   (    0.51 ms per token,  1942.09 tokens per second)\n",
        "llama_print_timings: prompt eval time =     387.68 ms /   101 tokens (    3.84 ms per token,   260.52 tokens per second)\n",
        "llama_print_timings:        eval time =   14551.79 ms /   255 runs   (   57.07 ms per token,    17.52 tokens per second)\n",
        "llama_print_timings:       total time =   15828.59 ms /   356 tokens\n",
        " 11%|\u2588         | 2/18 [00:32<04:18, 16.19s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     102.63 ms /   191 runs   (    0.54 ms per token,  1861.11 tokens per second)\n",
        "llama_print_timings: prompt eval time =     383.42 ms /    92 tokens (    4.17 ms per token,   239.95 tokens per second)\n",
        "llama_print_timings:        eval time =   10793.75 ms /   190 runs   (   56.81 ms per token,    17.60 tokens per second)\n",
        "llama_print_timings:       total time =   11839.43 ms /   282 tokens\n",
        " 17%|\u2588\u258b        | 3/18 [00:44<03:33, 14.20s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      32.73 ms /    58 runs   (    0.56 ms per token,  1772.07 tokens per second)\n",
        "llama_print_timings: prompt eval time =     474.83 ms /   133 tokens (    3.57 ms per token,   280.10 tokens per second)\n",
        "llama_print_timings:        eval time =    3229.72 ms /    57 runs   (   56.66 ms per token,    17.65 tokens per second)\n",
        "llama_print_timings:       total time =    3900.28 ms /   190 tokens\n",
        " 22%|\u2588\u2588\u258f       | 4/18 [00:48<02:21, 10.14s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      50.91 ms /    95 runs   (    0.54 ms per token,  1866.00 tokens per second)\n",
        "llama_print_timings: prompt eval time =     505.61 ms /   174 tokens (    2.91 ms per token,   344.14 tokens per second)\n",
        "llama_print_timings:        eval time =    5374.63 ms /    94 runs   (   57.18 ms per token,    17.49 tokens per second)\n",
        "llama_print_timings:       total time =    6200.40 ms /   268 tokens\n",
        " 28%|\u2588\u2588\u258a       | 5/18 [00:54<01:53,  8.72s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      99.68 ms /   182 runs   (    0.55 ms per token,  1825.84 tokens per second)\n",
        "llama_print_timings: prompt eval time =     387.92 ms /    99 tokens (    3.92 ms per token,   255.21 tokens per second)\n",
        "llama_print_timings:        eval time =   10321.00 ms /   181 runs   (   57.02 ms per token,    17.54 tokens per second)\n",
        "llama_print_timings:       total time =   11333.54 ms /   280 tokens\n",
        " 33%|\u2588\u2588\u2588\u258e      | 6/18 [01:05<01:55,  9.61s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     134.08 ms /   256 runs   (    0.52 ms per token,  1909.35 tokens per second)\n",
        "llama_print_timings: prompt eval time =     478.39 ms /   134 tokens (    3.57 ms per token,   280.11 tokens per second)\n",
        "llama_print_timings:        eval time =   14553.50 ms /   255 runs   (   57.07 ms per token,    17.52 tokens per second)\n",
        "llama_print_timings:       total time =   15925.91 ms /   389 tokens\n",
        " 39%|\u2588\u2588\u2588\u2589      | 7/18 [01:21<02:08, 11.68s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     130.67 ms /   256 runs   (    0.51 ms per token,  1959.16 tokens per second)\n",
        "llama_print_timings: prompt eval time =     468.36 ms /   131 tokens (    3.58 ms per token,   279.70 tokens per second)\n",
        "llama_print_timings:        eval time =   14540.57 ms /   255 runs   (   57.02 ms per token,    17.54 tokens per second)\n",
        "llama_print_timings:       total time =   15904.90 ms /   386 tokens\n",
        " 44%|\u2588\u2588\u2588\u2588\u258d     | 8/18 [01:37<02:10, 13.03s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     106.00 ms /   206 runs   (    0.51 ms per token,  1943.45 tokens per second)\n",
        "llama_print_timings: prompt eval time =     369.20 ms /    70 tokens (    5.27 ms per token,   189.60 tokens per second)\n",
        "llama_print_timings:        eval time =   11583.94 ms /   205 runs   (   56.51 ms per token,    17.70 tokens per second)\n",
        "llama_print_timings:       total time =   12670.52 ms /   275 tokens\n",
        " 50%|\u2588\u2588\u2588\u2588\u2588     | 9/18 [01:50<01:56, 12.92s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     131.14 ms /   256 runs   (    0.51 ms per token,  1952.14 tokens per second)\n",
        "llama_print_timings: prompt eval time =     471.88 ms /   131 tokens (    3.60 ms per token,   277.61 tokens per second)\n",
        "llama_print_timings:        eval time =   14511.00 ms /   255 runs   (   56.91 ms per token,    17.57 tokens per second)\n",
        "llama_print_timings:       total time =   15887.37 ms /   386 tokens\n",
        " 56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 10/18 [02:06<01:50, 13.84s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     135.80 ms /   256 runs   (    0.53 ms per token,  1885.11 tokens per second)\n",
        "llama_print_timings: prompt eval time =     371.11 ms /    73 tokens (    5.08 ms per token,   196.71 tokens per second)\n",
        "llama_print_timings:        eval time =   14443.45 ms /   255 runs   (   56.64 ms per token,    17.66 tokens per second)\n",
        "llama_print_timings:       total time =   15719.59 ms /   328 tokens\n",
        " 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 11/18 [02:21<01:40, 14.42s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      33.47 ms /    59 runs   (    0.57 ms per token,  1762.83 tokens per second)\n",
        "llama_print_timings: prompt eval time =     393.54 ms /   112 tokens (    3.51 ms per token,   284.60 tokens per second)\n",
        "llama_print_timings:        eval time =    3253.06 ms /    58 runs   (   56.09 ms per token,    17.83 tokens per second)\n",
        "llama_print_timings:       total time =    3853.10 ms /   170 tokens\n",
        " 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 12/18 [02:25<01:07, 11.21s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     135.08 ms /   256 runs   (    0.53 ms per token,  1895.22 tokens per second)\n",
        "llama_print_timings: prompt eval time =     393.22 ms /   109 tokens (    3.61 ms per token,   277.20 tokens per second)\n",
        "llama_print_timings:        eval time =   14521.98 ms /   255 runs   (   56.95 ms per token,    17.56 tokens per second)\n",
        "llama_print_timings:       total time =   15823.22 ms /   364 tokens\n",
        " 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 13/18 [02:41<01:03, 12.61s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     137.26 ms /   256 runs   (    0.54 ms per token,  1865.10 tokens per second)\n",
        "llama_print_timings: prompt eval time =     379.58 ms /    89 tokens (    4.26 ms per token,   234.47 tokens per second)\n",
        "llama_print_timings:        eval time =   14477.01 ms /   255 runs   (   56.77 ms per token,    17.61 tokens per second)\n",
        "llama_print_timings:       total time =   15776.83 ms /   344 tokens\n",
        " 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 14/18 [02:57<00:54, 13.57s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     105.56 ms /   198 runs   (    0.53 ms per token,  1875.71 tokens per second)\n",
        "llama_print_timings: prompt eval time =     489.43 ms /   152 tokens (    3.22 ms per token,   310.57 tokens per second)\n",
        "llama_print_timings:        eval time =   11236.37 ms /   197 runs   (   57.04 ms per token,    17.53 tokens per second)\n",
        "llama_print_timings:       total time =   12419.97 ms /   349 tokens\n",
        " 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 15/18 [03:09<00:39, 13.22s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     136.16 ms /   256 runs   (    0.53 ms per token,  1880.21 tokens per second)\n",
        "llama_print_timings: prompt eval time =     384.85 ms /    94 tokens (    4.09 ms per token,   244.25 tokens per second)\n",
        "llama_print_timings:        eval time =   14509.25 ms /   255 runs   (   56.90 ms per token,    17.57 tokens per second)\n",
        "llama_print_timings:       total time =   15806.26 ms /   349 tokens\n",
        " 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 16/18 [03:25<00:28, 14.00s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     125.07 ms /   256 runs   (    0.49 ms per token,  2046.87 tokens per second)\n",
        "llama_print_timings: prompt eval time =     376.41 ms /    80 tokens (    4.71 ms per token,   212.54 tokens per second)\n",
        "llama_print_timings:        eval time =   14488.57 ms /   255 runs   (   56.82 ms per token,    17.60 tokens per second)\n",
        "llama_print_timings:       total time =   15767.19 ms /   335 tokens\n",
        " 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 17/18 [03:41<00:14, 14.54s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     130.98 ms /   247 runs   (    0.53 ms per token,  1885.80 tokens per second)\n",
        "llama_print_timings: prompt eval time =     371.53 ms /    73 tokens (    5.09 ms per token,   196.49 tokens per second)\n",
        "llama_print_timings:        eval time =   13943.04 ms /   246 runs   (   56.68 ms per token,    17.64 tokens per second)\n",
        "llama_print_timings:       total time =   15193.13 ms /   319 tokens\n",
        "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [03:56<00:00, 13.15s/it]\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "0     None\n",
        "1     None\n",
        "2     None\n",
        "4     None\n",
        "6     None\n",
        "7     None\n",
        "9     None\n",
        "10    None\n",
        "11    None\n",
        "12    None\n",
        "15    None\n",
        "17    None\n",
        "18    None\n",
        "20    None\n",
        "21    None\n",
        "25    None\n",
        "26    None\n",
        "28    None\n",
        "Name: extracted_sentiment, dtype: object\n",
        "Combined Product Type and Aspect-Based Sentiment Accuracy: 0.0%\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "  0%|          | 0/18 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      82.13 ms /   165 runs   (    0.50 ms per token,  2008.94 tokens per second)\n",
        "llama_print_timings: prompt eval time =    1291.75 ms /   622 tokens (    2.08 ms per token,   481.52 tokens per second)\n",
        "llama_print_timings:        eval time =    9307.95 ms /   164 runs   (   56.76 ms per token,    17.62 tokens per second)\n",
        "llama_print_timings:       total time =   11172.50 ms /   786 tokens\n",
        "  6%|\u258c         | 1/18 [00:11<03:10, 11.18s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     132.88 ms /   256 runs   (    0.52 ms per token,  1926.58 tokens per second)\n",
        "llama_print_timings: prompt eval time =     387.67 ms /   101 tokens (    3.84 ms per token,   260.53 tokens per second)\n",
        "llama_print_timings:        eval time =   14465.39 ms /   255 runs   (   56.73 ms per token,    17.63 tokens per second)\n",
        "llama_print_timings:       total time =   15769.52 ms /   356 tokens\n",
        " 11%|\u2588         | 2/18 [00:26<03:42, 13.88s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     136.45 ms /   256 runs   (    0.53 ms per token,  1876.12 tokens per second)\n",
        "llama_print_timings: prompt eval time =     380.94 ms /    92 tokens (    4.14 ms per token,   241.51 tokens per second)\n",
        "llama_print_timings:        eval time =   14477.60 ms /   255 runs   (   56.77 ms per token,    17.61 tokens per second)\n",
        "llama_print_timings:       total time =   15775.73 ms /   347 tokens\n",
        " 17%|\u2588\u258b        | 3/18 [00:42<03:41, 14.75s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      80.34 ms /   160 runs   (    0.50 ms per token,  1991.49 tokens per second)\n",
        "llama_print_timings: prompt eval time =     475.15 ms /   133 tokens (    3.57 ms per token,   279.91 tokens per second)\n",
        "llama_print_timings:        eval time =    9028.59 ms /   159 runs   (   56.78 ms per token,    17.61 tokens per second)\n",
        "llama_print_timings:       total time =   10055.66 ms /   292 tokens\n",
        " 22%|\u2588\u2588\u258f       | 4/18 [00:52<03:00, 12.90s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      52.20 ms /    95 runs   (    0.55 ms per token,  1820.03 tokens per second)\n",
        "llama_print_timings: prompt eval time =     504.77 ms /   174 tokens (    2.90 ms per token,   344.71 tokens per second)\n",
        "llama_print_timings:        eval time =    5274.13 ms /    94 runs   (   56.11 ms per token,    17.82 tokens per second)\n",
        "llama_print_timings:       total time =    6108.32 ms /   268 tokens\n",
        " 28%|\u2588\u2588\u258a       | 5/18 [00:58<02:15, 10.45s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     131.20 ms /   245 runs   (    0.54 ms per token,  1867.32 tokens per second)\n",
        "llama_print_timings: prompt eval time =     387.05 ms /    99 tokens (    3.91 ms per token,   255.78 tokens per second)\n",
        "llama_print_timings:        eval time =   13833.49 ms /   244 runs   (   56.69 ms per token,    17.64 tokens per second)\n",
        "llama_print_timings:       total time =   15088.10 ms /   343 tokens\n",
        " 33%|\u2588\u2588\u2588\u258e      | 6/18 [01:14<02:24, 12.03s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     134.67 ms /   256 runs   (    0.53 ms per token,  1900.97 tokens per second)\n",
        "llama_print_timings: prompt eval time =     407.48 ms /   123 tokens (    3.31 ms per token,   301.85 tokens per second)\n",
        "llama_print_timings:        eval time =   14472.84 ms /   255 runs   (   56.76 ms per token,    17.62 tokens per second)\n",
        "llama_print_timings:       total time =   15799.05 ms /   378 tokens\n",
        " 39%|\u2588\u2588\u2588\u2589      | 7/18 [01:29<02:25, 13.27s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     130.28 ms /   256 runs   (    0.51 ms per token,  1965.06 tokens per second)\n",
        "llama_print_timings: prompt eval time =     471.50 ms /   134 tokens (    3.52 ms per token,   284.20 tokens per second)\n",
        "llama_print_timings:        eval time =   14457.47 ms /   255 runs   (   56.70 ms per token,    17.64 tokens per second)\n",
        "llama_print_timings:       total time =   15843.42 ms /   389 tokens\n",
        " 44%|\u2588\u2588\u2588\u2588\u258d     | 8/18 [01:45<02:20, 14.09s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     135.50 ms /   256 runs   (    0.53 ms per token,  1889.23 tokens per second)\n",
        "llama_print_timings: prompt eval time =     466.84 ms /   131 tokens (    3.56 ms per token,   280.61 tokens per second)\n",
        "llama_print_timings:        eval time =   14461.34 ms /   255 runs   (   56.71 ms per token,    17.63 tokens per second)\n",
        "llama_print_timings:       total time =   15848.62 ms /   386 tokens\n",
        " 50%|\u2588\u2588\u2588\u2588\u2588     | 9/18 [02:01<02:11, 14.64s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     134.64 ms /   256 runs   (    0.53 ms per token,  1901.39 tokens per second)\n",
        "llama_print_timings: prompt eval time =     372.53 ms /    73 tokens (    5.10 ms per token,   195.96 tokens per second)\n",
        "llama_print_timings:        eval time =   14501.40 ms /   255 runs   (   56.87 ms per token,    17.58 tokens per second)\n",
        "llama_print_timings:       total time =   15778.94 ms /   328 tokens\n",
        " 56%|\u2588\u2588\u2588\u2588\u2588\u258c    | 10/18 [02:17<01:59, 15.00s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      48.16 ms /    91 runs   (    0.53 ms per token,  1889.34 tokens per second)\n",
        "llama_print_timings: prompt eval time =     394.36 ms /   112 tokens (    3.52 ms per token,   284.01 tokens per second)\n",
        "llama_print_timings:        eval time =    5067.13 ms /    90 runs   (   56.30 ms per token,    17.76 tokens per second)\n",
        "llama_print_timings:       total time =    5776.17 ms /   202 tokens\n",
        " 61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 11/18 [02:23<01:25, 12.18s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     138.93 ms /   256 runs   (    0.54 ms per token,  1842.68 tokens per second)\n",
        "llama_print_timings: prompt eval time =     409.07 ms /   121 tokens (    3.38 ms per token,   295.80 tokens per second)\n",
        "llama_print_timings:        eval time =   14507.86 ms /   255 runs   (   56.89 ms per token,    17.58 tokens per second)\n",
        "llama_print_timings:       total time =   15842.61 ms /   376 tokens\n",
        " 67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 12/18 [02:38<01:19, 13.29s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     135.72 ms /   256 runs   (    0.53 ms per token,  1886.28 tokens per second)\n",
        "llama_print_timings: prompt eval time =     379.05 ms /    89 tokens (    4.26 ms per token,   234.80 tokens per second)\n",
        "llama_print_timings:        eval time =   14507.08 ms /   255 runs   (   56.89 ms per token,    17.58 tokens per second)\n",
        "llama_print_timings:       total time =   15797.57 ms /   344 tokens\n",
        " 72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 13/18 [02:54<01:10, 14.06s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     127.66 ms /   243 runs   (    0.53 ms per token,  1903.52 tokens per second)\n",
        "llama_print_timings: prompt eval time =     494.94 ms /   152 tokens (    3.26 ms per token,   307.11 tokens per second)\n",
        "llama_print_timings:        eval time =   13756.58 ms /   242 runs   (   56.85 ms per token,    17.59 tokens per second)\n",
        "llama_print_timings:       total time =   15118.30 ms /   394 tokens\n",
        " 78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a  | 14/18 [03:09<00:57, 14.38s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     117.17 ms /   221 runs   (    0.53 ms per token,  1886.15 tokens per second)\n",
        "llama_print_timings: prompt eval time =     400.36 ms /   121 tokens (    3.31 ms per token,   302.23 tokens per second)\n",
        "llama_print_timings:        eval time =   12513.18 ms /   220 runs   (   56.88 ms per token,    17.58 tokens per second)\n",
        "llama_print_timings:       total time =   13701.38 ms /   341 tokens\n",
        " 83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 15/18 [03:23<00:42, 14.18s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     134.40 ms /   256 runs   (    0.53 ms per token,  1904.75 tokens per second)\n",
        "llama_print_timings: prompt eval time =     376.31 ms /    80 tokens (    4.70 ms per token,   212.59 tokens per second)\n",
        "llama_print_timings:        eval time =   14424.90 ms /   255 runs   (   56.57 ms per token,    17.68 tokens per second)\n",
        "llama_print_timings:       total time =   15714.56 ms /   335 tokens\n",
        " 89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 16/18 [03:39<00:29, 14.64s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     136.55 ms /   256 runs   (    0.53 ms per token,  1874.81 tokens per second)\n",
        "llama_print_timings: prompt eval time =     371.42 ms /    73 tokens (    5.09 ms per token,   196.54 tokens per second)\n",
        "llama_print_timings:        eval time =   14417.47 ms /   255 runs   (   56.54 ms per token,    17.69 tokens per second)\n",
        "llama_print_timings:       total time =   15716.53 ms /   328 tokens\n",
        " 94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 17/18 [03:55<00:14, 14.97s/it]Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      93.42 ms /   177 runs   (    0.53 ms per token,  1894.61 tokens per second)\n",
        "llama_print_timings: prompt eval time =     407.67 ms /   128 tokens (    3.18 ms per token,   313.98 tokens per second)\n",
        "llama_print_timings:        eval time =    9984.13 ms /   176 runs   (   56.73 ms per token,    17.63 tokens per second)\n",
        "llama_print_timings:       total time =   11025.24 ms /   304 tokens\n",
        "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [04:06<00:00, 13.67s/it]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "0     None\n",
        "1     None\n",
        "2     None\n",
        "4     None\n",
        "6     None\n",
        "7     None\n",
        "8     None\n",
        "9     None\n",
        "10    None\n",
        "15    None\n",
        "17    None\n",
        "19    None\n",
        "20    None\n",
        "21    None\n",
        "22    None\n",
        "26    None\n",
        "28    None\n",
        "29    None\n",
        "Name: extracted_sentiment, dtype: object\n",
        "Combined Product Type and Aspect-Based Sentiment Accuracy: 0.0%\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_list\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "[0.0, 0.0, 0.0]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean accuracy - sum of values in \"accuracy list\" divided by total values in the \"accuracy_list\"\n",
        "mean_accuracy = sum(accuracy_list) / len(accuracy_list)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "mean_accuracy\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "0.0"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gold_examples\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'[{\"review_text\":\"This is a very bad power bank, it does not charge my phone properly. It takes a very long time to charge the power bank itself, and it drains very fast. It also does not charge my phone fully, it stops at around 80%. It also heats up very much, and sometimes it sparks and smokes. I think it is very dangerous and defective. I tried to return it, but the seller did not accept it. I feel cheated and scammed.\",\"product_type\":\"Power Bank\",\"aspects_review\":\"{ battery : Negative , charging : Negative }\"},{\"review_text\":\"This is a great laptop, I am very happy with it. Great battery life, it lasts for about 8 hours. It has great performance, it can handle multiple tasks and applications. Good storage capacity, it can store a lot of files and data. The laptop also has a great screen, it has a good resolution and viewing angle. It also has a great design, it\\'s sturdy and durable and the keyboard\\'s keys are good and strong. Overall I found it perfect, with basically no flaws at all. Great buy!\",\"product_type\":\"Laptop\",\"aspects_review\":\"{ battery : Positive , keyboard : Positive , display : Positive }\"},{\"review_text\":\"Great for listening to music or podcasts. Good sound quality, battery life and charging speed. Quite comfortable too. Highly recommended!\",\"product_type\":\"Headphones\",\"aspects_review\":\"{ sound : Positive , comfort : Positive , charging : Positive }\"},{\"review_text\":\"Good phone, I am satisfied with it. The phone has a good design, it is slim and light. The screen is also big and bright, it has a high resolution and a smooth refresh rate. The phone is also fast and smooth, it has a powerful processor and a large memory. The phone also has a good camera, it takes good pictures and videos. The battery life is also good, it lasts for a whole day. The phone also has a lot of features and functions, like wireless charging, fingerprint scanner, face recognition, and more. I think this phone is a good buy.\",\"product_type\":\"Smartphone\",\"aspects_review\":\"{ display : Positive , camera : Positive , Battery : Positive }\"},{\"review_text\":\"Originally bought it for my work, quite happy with it so far! Fast, reliable, easy to use and has a good webcam. Display is good and battery backup is also great. The keyboard is a joy to type on, gives me the old typewriter vibes! Quickly become my main laptop for everyday use, and I\\'m very satisfied with my purchase.\",\"product_type\":\"Laptop\",\"aspects_review\":\"{ battery : Positive , keyboard : Positive , display : Positive }\"},{\"review_text\":\"I bought this phone mainly for its much-hyped camera, but I was very disappointed with the results. The pictures are blurry, grainy, and overexposed. The zoom is also very bad, it makes the pictures look pixelated and distorted. The night mode is also useless, it makes the pictures look dark and noisy. The video quality is also very poor, it lags and stutters. Battery is also not very good. Only good thing is display. I expected much better from a flagship phone.\",\"product_type\":\"Smartphone\",\"aspects_review\":\"{ display : Positive , camera : Negative , Battery : Negative }\"},{\"review_text\":\"I bought these headphones just 2 weeks ago, and I already regret it. The sound quality is quite poor, and charging takes too long. The headphones are really uncomfortable to wear, they hurt my ears and head. The headphones are also very tight and heavy, they make me sweat and itch. They also leak sound and disturb others, can you believe that? I do not like these headphones, they are a pain.\",\"product_type\":\"Headphones\",\"aspects_review\":\"{ sound : Negative , comfort : Negative , charging : Negative }\"},{\"review_text\":\"I love this power bank - it\\'s amazing. It can charge my phone several times and it is very compact and portable. It also has a fast charging feature, which is very convenient. The power bank is durable and has a sleek design. I am very satisfied with this product and I would highly recommend it.\",\"product_type\":\"Power Bank\",\"aspects_review\":\"{ battery : Positive , charging : Positive }\"}]'"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_prompt(prompt, gold_examples, user_message_template):\n",
        "\n",
        "    \"\"\"\n",
        "    Return the micro-F1 score for predictions on gold examples.\n",
        "    For each example, we make a prediction using the prompt. Gold labels and\n",
        "    model predictions are aggregated into lists and compared to compute the\n",
        "    F1 score.\n",
        "\n",
        "    Args:\n",
        "        prompt (List): list of messages in the Open AI prompt format\n",
        "        gold_examples (str): JSON string with list of gold examples\n",
        "        user_message_template (str): string with a placeholder for movie reviews\n",
        "\n",
        "    Output:\n",
        "        micro_f1_score (float): Micro-F1 score computed by comparing model predictions\n",
        "                                with ground truth\n",
        "    \"\"\"\n",
        "\n",
        "    model_predictions, ground_truths = [], []\n",
        "\n",
        "    for example in json.loads(gold_examples):\n",
        "        gold_input = example['review_text']\n",
        "        user_input = [\n",
        "            {\n",
        "               user_message_template.format(review=gold_input)\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            prediction = generate_llama_response( few_shot_system_message , few_shot_examples_str  , user_input , 0.1 )\n",
        "\n",
        "            print(\"prediction : \" + prediction + \"\\n\")\n",
        "\n",
        "            prediction_extracted = extract_sentiment(prediction)\n",
        "            predicted_product_type = extract_product_type(prediction_extracted)\n",
        "            predicted_aspect_based_sentiment = extract_aspect_based_sentiment(prediction_extracted)\n",
        "\n",
        "            final_prediction = predicted_product_type + \":\" + predicted_aspect_based_sentiment\n",
        "\n",
        "            # sentiment = match.group().lower() if match else \"Sentiment not found.\"\n",
        "            print(\"model_prediction : \" + final_prediction.lower() + \"\\n\")\n",
        "\n",
        "            model_predictions.append(final_prediction.lower()) # <- removes extraneous white space and lowercases output\n",
        "            ground_truths.append( example['product_type'].lower() + \":\" + example['aspects_review'].lower() )\n",
        "\n",
        "            print(\"ground truth : \" + example['product_type'].lower() + \":\" + example['aspects_review'].lower() + \"\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    micro_f1_score = f1_score(ground_truths, model_predictions, average=\"micro\")\n",
        "\n",
        "    return micro_f1_score\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the results (2 Marks)\n",
        "evaluate_prompt(few_shot_system_message, gold_examples, user_message_template)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Llama.generate: prefix-match hit\n",
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     135.92 ms /   256 runs   (    0.53 ms per token,  1883.52 tokens per second)\n",
        "llama_print_timings: prompt eval time =     430.79 ms /   110 tokens (    3.92 ms per token,   255.34 tokens per second)\n",
        "llama_print_timings:        eval time =   12997.09 ms /   255 runs   (   50.97 ms per token,    19.62 tokens per second)\n",
        "llama_print_timings:       total time =   14348.13 ms /   365 tokens\n",
        "Llama.generate: prefix-match hit\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prediction :   Sure! Here are the sentiment analysis results for each product based on the reviews you provided:\n",
        "\n",
        "1. Headphones: The review expresses a negative sentiment towards the headphones, with complaints about the sound quality, comfort, charging, and static noise. Therefore, the sentiment analysis result for this product is \"Negative\".\n",
        "2. Smartphone: The review expresses a negative sentiment towards the smartphone, with complaints about the battery life, display, camera, and customer service. Therefore, the sentiment analysis result for this product is also \"Negative\".\n",
        "3. Laptop: The review expresses a mixed sentiment towards the laptop, with both positive and negative comments. However, since there are more negative comments than positive ones, the overall sentiment analysis result for this product is \"Neutral\".\n",
        "4. Power bank: The review expresses a mixed sentiment towards the power bank, with both positive and negative comments. However, since there are more positive comments than negative ones, the overall sentiment analysis result for this product is \"Positive\".\n",
        "\n",
        "Here's the summary of the results in a table format:\n",
        "\n",
        "| Product | Sentiment Analysis Result |\n",
        "| --- | --- |\n",
        "|\n",
        "\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     136.66 ms /   256 runs   (    0.53 ms per token,  1873.23 tokens per second)\n",
        "llama_print_timings: prompt eval time =     396.98 ms /   122 tokens (    3.25 ms per token,   307.32 tokens per second)\n",
        "llama_print_timings:        eval time =   13248.38 ms /   255 runs   (   51.95 ms per token,    19.25 tokens per second)\n",
        "llama_print_timings:       total time =   14570.05 ms /   377 tokens\n",
        "Llama.generate: prefix-match hit\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prediction :   Sure! Here is the natural language processing of the given text:\n",
        "\n",
        "### Example 1\n",
        "{\n",
        "'review_text': \"These headphones are amazing! The sound quality is excellent, and the comfort is top-notch.\",\n",
        "'sentiment': 'Positive',\n",
        "'product_type': 'Headphones'\n",
        "}\n",
        "\n",
        "* Sentiment: Positive\n",
        "* Product type: Headphones\n",
        "\n",
        "### Example 2\n",
        "{\n",
        "'review_text': \"The battery life on this smartphone is really disappointing. It barely lasts a day.\",\n",
        "'sentiment': 'Negative',\n",
        "'product_type': 'Smartphone'\n",
        "}\n",
        "\n",
        "* Sentiment: Negative\n",
        "* Product type: Smartphone\n",
        "\n",
        "### Example 3\n",
        "{\n",
        "'review_text': \"This laptop is great for the price. It performs well for everyday tasks, but the screen resolution could be better.\",\n",
        "'sentiment': 'Neutral',\n",
        "'product_type': 'Laptop'\n",
        "}\n",
        "\n",
        "* Sentiment: Neutral\n",
        "* Product type: Laptop\n",
        "\n",
        "### Example 4\n",
        "{\n",
        "\n",
        "\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     132.97 ms /   252 runs   (    0.53 ms per token,  1895.11 tokens per second)\n",
        "llama_print_timings: prompt eval time =     337.38 ms /    37 tokens (    9.12 ms per token,   109.67 tokens per second)\n",
        "llama_print_timings:        eval time =   12993.56 ms /   251 runs   (   51.77 ms per token,    19.32 tokens per second)\n",
        "llama_print_timings:       total time =   14241.05 ms /   288 tokens\n",
        "Llama.generate: prefix-match hit\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prediction :   Sure! Here are the sentiment analysis results based on the examples you provided:\n",
        "\n",
        "1. Example 1 (Headphones):\n",
        "\t* Sentiment: Positive\n",
        "\t* Reasons: Good sound quality, battery life, and charging speed; comfortable to wear\n",
        "2. Example 2 (Smartphone):\n",
        "\t* Sentiment: Negative\n",
        "\t* Reasons: Poor battery life, display, camera, and customer service; does not support fast charging\n",
        "3. Example 3 (Laptop):\n",
        "\t* Sentiment: Neutral\n",
        "\t* Reasons: Good keyboard quality, but poor screen resolution and glare\n",
        "4. Example 4 (Power Bank):\n",
        "\t* Sentiment: Positive\n",
        "\t* Reasons: Good battery backup, useful with dual USB ports; easy pick-up and drop-off service\n",
        "5. Example 5 (Smartphone):\n",
        "\t* Sentiment: Negative\n",
        "\t* Reasons: Poor battery life, display, camera, and customer service; heats up quickly and has a poor charging speed\n",
        "\n",
        "I hope this helps! Let me know if you have any other questions or need further assistance.\n",
        "\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      98.13 ms /   185 runs   (    0.53 ms per token,  1885.31 tokens per second)\n",
        "llama_print_timings: prompt eval time =     462.00 ms /   131 tokens (    3.53 ms per token,   283.55 tokens per second)\n",
        "llama_print_timings:        eval time =    9712.00 ms /   184 runs   (   52.78 ms per token,    18.95 tokens per second)\n",
        "llama_print_timings:       total time =   10834.12 ms /   315 tokens\n",
        "Llama.generate: prefix-match hit\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prediction :   Sure! Here are the sentiment analysis results based on the examples you provided:\n",
        "\n",
        "Example 1:\n",
        "\n",
        "* Sentiment: Positive\n",
        "* Product features mentioned: sound quality, comfort\n",
        "\n",
        "Example 2:\n",
        "\n",
        "* Sentiment: Negative\n",
        "* Product features mentioned: battery life\n",
        "\n",
        "Example 3:\n",
        "\n",
        "* Sentiment: Neutral\n",
        "* Product features mentioned: performance, screen resolution\n",
        "\n",
        "Based on these examples, it seems that the sentiment analysis model is able to accurately identify positive and negative sentiments towards products based on the features mentioned in the reviews. However, there may be some limitations to this approach, such as difficulty in identifying neutral sentiments or nuanced opinions that do not fit neatly into positive or negative categories. Additionally, the accuracy of the sentiment analysis may depend on the quality and quantity of training data used to train the model.\n",
        "\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     139.30 ms /   256 runs   (    0.54 ms per token,  1837.79 tokens per second)\n",
        "llama_print_timings: prompt eval time =     375.46 ms /    85 tokens (    4.42 ms per token,   226.39 tokens per second)\n",
        "llama_print_timings:        eval time =   13614.64 ms /   255 runs   (   53.39 ms per token,    18.73 tokens per second)\n",
        "llama_print_timings:       total time =   14916.78 ms /   340 tokens\n",
        "Llama.generate: prefix-match hit\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prediction :   Sure, I can help you with that! Here are the sentiment analysis results for each product based on the reviews provided:\n",
        "\n",
        "1. Headphones: The review is positive overall, but there are some negative aspects mentioned (e.g., slipping, static noise). Therefore, the sentiment score would be around 80% positive and 20% negative.\n",
        "2. Smartphone: The review is overwhelmingly negative, with several complaints about battery life, display quality, camera performance, charging speed, and customer service. Based on this information, I would assign a sentiment score of around 5% positive and 95% negative.\n",
        "3. Laptop: The review has mixed feelings about the laptop's screen resolution and keyboard, but overall it is satisfied with its purchase. Therefore, the sentiment score would be around 70% positive and 30% neutral.\n",
        "4. Power bank: The review mentions both positive (battery backup) and negative aspects (charging speed), so the sentiment score would be around 60% positive and 40% neutral.\n",
        "5. Smartphone (again): This review is extremely negative, with no redeeming\n",
        "\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     135.07 ms /   256 runs   (    0.53 ms per token,  1895.36 tokens per second)\n",
        "llama_print_timings: prompt eval time =     398.49 ms /   121 tokens (    3.29 ms per token,   303.65 tokens per second)\n",
        "llama_print_timings:        eval time =   14064.01 ms /   255 runs   (   55.15 ms per token,    18.13 tokens per second)\n",
        "llama_print_timings:       total time =   15393.77 ms /   376 tokens\n",
        "Llama.generate: prefix-match hit\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prediction :   Sure! Here are the sentiment analysis results for each product based on the reviews you provided:\n",
        "\n",
        "1. Headphones:\n",
        "\t* Sentiment: Negative\n",
        "\t* Reasons: poor sound quality, uncomfortable, buggy charging, static noise, interference\n",
        "2. Smartphone:\n",
        "\t* Sentiment: Very Negative\n",
        "\t* Reasons: bad battery life, below-average display, poor camera, slow charging, overheating, rude customer service\n",
        "3. Laptop:\n",
        "\t* Sentiment: Neutral\n",
        "\t* Reasons: good keyboard, good battery life, but poor screen quality (dim, dull, dead pixels)\n",
        "4. Power Bank:\n",
        "\t* Sentiment: Mixed\n",
        "\t* Reasons: good battery backup, useful with dual USB ports, but inconsistent charging speed\n",
        "5. Smartphone (second review):\n",
        "\t* Sentiment: Very Negative\n",
        "\t* Reasons: poor camera quality, blurry and grainy pictures, useless night mode, laggy and stuttering video, bad battery life\n",
        "\n",
        "Note that the sentiment analysis is based on the reviews provided and may not reflect the actual experiences\n",
        "\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =     136.91 ms /   256 runs   (    0.53 ms per token,  1869.91 tokens per second)\n",
        "llama_print_timings: prompt eval time =     386.68 ms /   102 tokens (    3.79 ms per token,   263.79 tokens per second)\n",
        "llama_print_timings:        eval time =   14113.46 ms /   255 runs   (   55.35 ms per token,    18.07 tokens per second)\n",
        "llama_print_timings:       total time =   15413.57 ms /   357 tokens\n",
        "Llama.generate: prefix-match hit\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prediction :   Sure! Here's the natural language processing (NLP) analysis of the user reviews:\n",
        "\n",
        "1. Headphones Review:\n",
        "\t* Sentiment Analysis: Negative\n",
        "\t* Reasons for negative sentiment: Poor sound quality, uncomfortable to wear, charging takes too long\n",
        "2. Smartphone Review:\n",
        "\t* Sentiment Analysis: Very Negative\n",
        "\t* Reasons for negative sentiment: Bad battery life, poor display, bad camera, slow charging, heats up quickly\n",
        "3. Laptop Review:\n",
        "\t* Sentiment Analysis: Neutral\n",
        "\t* Reasons for neutral sentiment: Good keyboard and battery life, but poor screen resolution\n",
        "4. Power Bank Review:\n",
        "\t* Sentiment Analysis: Positive\n",
        "\t* Reasons for positive sentiment: Good battery backup, useful with dual USB ports, easy pick-up and drop-off service\n",
        "\n",
        "Based on the reviews, it seems that the headphones and smartphone have received negative sentiments due to various issues such as poor sound quality, uncomfortable wear, slow charging, and bad display. The laptop has received a neutral sentiment as it has both positive and negative features. The power bank has\n",
        "\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llama_print_timings:        load time =     948.24 ms\n",
        "llama_print_timings:      sample time =      95.35 ms /   185 runs   (    0.52 ms per token,  1940.16 tokens per second)\n",
        "llama_print_timings: prompt eval time =     372.39 ms /    75 tokens (    4.97 ms per token,   201.40 tokens per second)\n",
        "llama_print_timings:        eval time =   10112.45 ms /   184 runs   (   54.96 ms per token,    18.20 tokens per second)\n",
        "llama_print_timings:       total time =   11141.48 ms /   259 tokens\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "prediction :   Sure! Here are the sentiment analysis results for each example:\n",
        "\n",
        "Example 1:\n",
        "\n",
        "* Sentiment: Positive\n",
        "* Product type: Headphones\n",
        "\n",
        "Example 2:\n",
        "\n",
        "* Sentiment: Negative\n",
        "* Product type: Smartphone\n",
        "\n",
        "Example 3:\n",
        "\n",
        "* Sentiment: Neutral\n",
        "* Product type: Laptop\n",
        "\n",
        "Example 4:\n",
        "\n",
        "* Sentiment: Mixed (both positive and negative)\n",
        "* Product type: Power bank\n",
        "\n",
        "Example 5:\n",
        "\n",
        "* Sentiment: Negative\n",
        "* Product type: Smartphone\n",
        "\n",
        "Based on the sentiment analysis, it seems that users are generally more satisfied with products that have good battery life and performance, but may be less satisfied with products that have poor display or charging issues. Additionally, some users may have had negative experiences with customer service.\n",
        "\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "0.0"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        " \n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth\n",
        "!pip install -q --no-deps xformers trl peft accelerate bitsandbytes\n",
        "!pip install -q datasets evaluate rouge_score bert_score\n",
        "!pip freeze>r.txt\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Collecting unsloth\n",
        "  Downloading unsloth-2024.9.post3-py3-none-any.whl (165 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 165.6/165.6 KB 5.9 MB/s eta 0:00:00\n",
        "Collecting wheel>=0.42.0\n",
        "  Downloading wheel-0.44.0-py3-none-any.whl (67 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 67.1/67.1 KB 25.9 MB/s eta 0:00:00\n",
        "Collecting huggingface-hub\n",
        "  Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 436.4/436.4 KB 52.1 MB/s eta 0:00:00\n",
        "Collecting xformers>=0.0.27.post2\n",
        "  Downloading xformers-0.0.28.post1-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 16.7/16.7 MB 83.4 MB/s eta 0:00:0000:0100:01\n",
        "Collecting triton>=3.0.0\n",
        "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 209.4/209.4 MB 9.0 MB/s eta 0:00:0000:0100:01\n",
        "Collecting hf-transfer\n",
        "  Downloading hf_transfer-0.1.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 3.6/3.6 MB 116.6 MB/s eta 0:00:00\n",
        "Collecting tyro\n",
        "  Downloading tyro-0.8.11-py3-none-any.whl (105 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 105.9/105.9 KB 32.5 MB/s eta 0:00:00\n",
        "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from unsloth) (24.1)\n",
        "Collecting protobuf<4.0.0\n",
        "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.1/1.1 MB 97.6 MB/s eta 0:00:00\n",
        "Collecting tqdm\n",
        "  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 78.4/78.4 KB 26.7 MB/s eta 0:00:00\n",
        "Collecting datasets>=2.16.0\n",
        "  Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 471.6/471.6 KB 87.5 MB/s eta 0:00:00\n",
        "Collecting torch>=2.4.0\n",
        "  Downloading torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.1/797.1 MB 1.5 MB/s eta 0:00:0000:0100:01\n",
        "Collecting bitsandbytes\n",
        "  Downloading bitsandbytes-0.44.0-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 122.4/122.4 MB 15.5 MB/s eta 0:00:0000:0100:01\n",
        "Collecting accelerate>=0.26.1\n",
        "  Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 324.4/324.4 KB 62.5 MB/s eta 0:00:00\n",
        "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unsloth) (1.26.4)\n",
        "Collecting transformers>=4.45.0\n",
        "  Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.9/9.9 MB 94.4 MB/s eta 0:00:00:00:0100:01\n",
        "Collecting peft!=0.11.0,>=0.7.1\n",
        "  Downloading peft-0.13.0-py3-none-any.whl (322 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 322.5/322.5 KB 53.3 MB/s eta 0:00:00\n",
        "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unsloth) (6.0.0)\n",
        "Collecting sentencepiece>=0.2.0\n",
        "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.3/1.3 MB 69.6 MB/s eta 0:00:00\n",
        "Collecting trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9\n",
        "  Downloading trl-0.11.1-py3-none-any.whl (318 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 318.4/318.4 KB 51.3 MB/s eta 0:00:00\n",
        "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.1->unsloth) (6.0.2)\n",
        "Collecting safetensors>=0.4.3\n",
        "  Downloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 435.0/435.0 KB 76.6 MB/s eta 0:00:00\n",
        "Collecting multiprocess\n",
        "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.8/134.8 KB 37.1 MB/s eta 0:00:00\n",
        "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n",
        "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (15.0.2)\n",
        "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (3.15.4)\n",
        "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
        "Collecting xxhash\n",
        "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 194.1/194.1 KB 51.7 MB/s eta 0:00:00\n",
        "Collecting aiohttp\n",
        "  Downloading aiohttp-3.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.2/1.2 MB 108.6 MB/s eta 0:00:00\n",
        "Collecting dill<0.3.9,>=0.3.0\n",
        "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 116.3/116.3 KB 38.2 MB/s eta 0:00:00\n",
        "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->unsloth) (2024.6.1)\n",
        "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->unsloth) (4.12.2)\n",
        "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.1.0.106)\n",
        "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (2.20.5)\n",
        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
        "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.1.3.1)\n",
        "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (11.0.2.54)\n",
        "Collecting nvidia-cudnn-cu12==9.1.0.70\n",
        "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 664.8/664.8 MB 1.8 MB/s eta 0:00:0000:0100:01\n",
        "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (11.4.5.107)\n",
        "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (10.3.2.106)\n",
        "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.1.105)\n",
        "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.1.105)\n",
        "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.1.105)\n",
        "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (1.13.2)\n",
        "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (12.1.105)\n",
        "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->unsloth) (3.3)\n",
        "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.4.0->unsloth) (12.6.20)\n",
        "Collecting regex!=2019.12.17\n",
        "  Downloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 782.7/782.7 KB 107.3 MB/s eta 0:00:00\n",
        "Collecting tokenizers<0.21,>=0.20\n",
        "  Downloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.9/2.9 MB 134.5 MB/s eta 0:00:00\n",
        "Collecting docstring-parser>=0.16\n",
        "  Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
        "Collecting shtab>=1.5.6\n",
        "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
        "Collecting rich>=11.1.0\n",
        "  Downloading rich-13.8.1-py3-none-any.whl (241 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 241.6/241.6 KB 40.1 MB/s eta 0:00:00\n",
        "Collecting frozenlist>=1.1.1\n",
        "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 239.5/239.5 KB 54.3 MB/s eta 0:00:00\n",
        "Collecting yarl<2.0,>=1.12.0\n",
        "  Downloading yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (447 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 447.9/447.9 KB 87.8 MB/s eta 0:00:00\n",
        "Collecting async-timeout<5.0,>=4.0\n",
        "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
        "Collecting aiosignal>=1.1.2\n",
        "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
        "Collecting multidict<7.0,>=4.5\n",
        "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 124.6/124.6 KB 22.5 MB/s eta 0:00:00\n",
        "Collecting aiohappyeyeballs>=2.3.0\n",
        "  Downloading aiohappyeyeballs-2.4.2-py3-none-any.whl (14 kB)\n",
        "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->unsloth) (24.2.0)\n",
        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.2.2)\n",
        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.8)\n",
        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.7.4)\n",
        "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.3.2)\n",
        "Collecting markdown-it-py>=2.2.0\n",
        "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 87.5/87.5 KB 18.6 MB/s eta 0:00:00\n",
        "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.18.0)\n",
        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->unsloth) (2.1.5)\n",
        "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.1)\n",
        "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2024.1)\n",
        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.4.0->unsloth) (1.3.0)\n",
        "Collecting mdurl~=0.1\n",
        "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
        "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n",
        "Installing collected packages: sentencepiece, xxhash, wheel, triton, tqdm, shtab, safetensors, regex, protobuf, nvidia-cudnn-cu12, multidict, mdurl, hf-transfer, frozenlist, docstring-parser, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, markdown-it-py, huggingface-hub, aiosignal, torch, tokenizers, rich, aiohttp, xformers, tyro, transformers, bitsandbytes, accelerate, peft, datasets, trl, unsloth\n",
        "  Attempting uninstall: wheel\n",
        "    Found existing installation: wheel 0.37.1\n",
        "    Not uninstalling wheel at /usr/lib/python3/dist-packages, outside environment /usr\n",
        "    Can't uninstall 'wheel'. No files were found to uninstall.\n",
        "  Attempting uninstall: triton\n",
        "    Found existing installation: triton 2.3.1\n",
        "    Uninstalling triton-2.3.1:\n",
        "      Successfully uninstalled triton-2.3.1\n",
        "  Attempting uninstall: protobuf\n",
        "    Found existing installation: protobuf 4.25.4\n",
        "    Uninstalling protobuf-4.25.4:\n",
        "      Successfully uninstalled protobuf-4.25.4\n",
        "  Attempting uninstall: nvidia-cudnn-cu12\n",
        "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
        "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
        "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
        "  Attempting uninstall: torch\n",
        "    Found existing installation: torch 2.3.1\n",
        "    Uninstalling torch-2.3.1:\n",
        "      Successfully uninstalled torch-2.3.1\n",
        "Successfully installed accelerate-0.34.2 aiohappyeyeballs-2.4.2 aiohttp-3.10.8 aiosignal-1.3.1 async-timeout-4.0.3 bitsandbytes-0.44.0 datasets-3.0.1 dill-0.3.8 docstring-parser-0.16 frozenlist-1.4.1 hf-transfer-0.1.8 huggingface-hub-0.25.1 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.1.0 multiprocess-0.70.16 nvidia-cudnn-cu12-9.1.0.70 peft-0.13.0 protobuf-3.20.3 regex-2024.9.11 rich-13.8.1 safetensors-0.4.5 sentencepiece-0.2.0 shtab-1.7.1 tokenizers-0.20.0 torch-2.4.1 tqdm-4.66.5 transformers-4.45.1 triton-3.0.0 trl-0.11.1 tyro-0.8.11 unsloth-2024.9.post3 wheel-0.44.0 xformers-0.0.28.post1 xxhash-3.5.0 yarl-1.13.1\n",
        "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
        "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
        "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipywidgets --upgrade\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Collecting ipywidgets\n",
        "  Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 139.8/139.8 KB 4.9 MB/s eta 0:00:00\n",
        "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.14.3)\n",
        "Collecting widgetsnbextension~=4.0.12\n",
        "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.3/2.3 MB 83.7 MB/s eta 0:00:00\n",
        "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.26.0)\n",
        "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\n",
        "Collecting jupyterlab-widgets~=3.0.12\n",
        "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 214.4/214.4 KB 52.0 MB/s eta 0:00:00\n",
        "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
        "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
        "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
        "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
        "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
        "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
        "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
        "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
        "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
        "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
        "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
        "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
        "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
        "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
        "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
        "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
        "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
        "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
        "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jupyter --upgrade\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Collecting jupyter\n",
        "  Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
        "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter) (7.16.4)\n",
        "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.5.7)\n",
        "Collecting jupyterlab\n",
        "  Downloading jupyterlab-4.2.5-py3-none-any.whl (11.6 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 11.6/11.6 MB 95.4 MB/s eta 0:00:0000:0100:01\n",
        "Requirement already satisfied: ipykernel in /usr/local/lib/python3.10/dist-packages (from jupyter) (6.29.5)\n",
        "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (from jupyter) (8.1.5)\n",
        "Collecting jupyter-console\n",
        "  Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)\n",
        "Requirement already satisfied: comm>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (0.2.2)\n",
        "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (7.4.9)\n",
        "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (26.2.0)\n",
        "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (0.1.7)\n",
        "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (6.4.1)\n",
        "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (24.1)\n",
        "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (8.26.0)\n",
        "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (5.7.2)\n",
        "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (1.6.0)\n",
        "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (5.14.3)\n",
        "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (6.0.0)\n",
        "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel->jupyter) (1.8.5)\n",
        "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter) (3.0.13)\n",
        "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets->jupyter) (4.0.13)\n",
        "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter) (2.18.0)\n",
        "Requirement already satisfied: prompt-toolkit>=3.0.30 in /usr/local/lib/python3.10/dist-packages (from jupyter-console->jupyter) (3.0.47)\n",
        "Requirement already satisfied: setuptools>=40.1.0 in /usr/lib/python3/dist-packages (from jupyterlab->jupyter) (59.6.0)\n",
        "Collecting async-lru>=1.0.0\n",
        "  Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
        "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (2.14.2)\n",
        "Collecting jupyterlab-server<3,>=2.27.1\n",
        "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 59.7/59.7 KB 19.3 MB/s eta 0:00:00\n",
        "Collecting tomli>=1.2.2\n",
        "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
        "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (3.1.4)\n",
        "Collecting jupyter-lsp>=2.0.0\n",
        "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 69.1/69.1 KB 28.6 MB/s eta 0:00:00\n",
        "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.10/dist-packages (from jupyterlab->jupyter) (0.2.4)\n",
        "Collecting httpx>=0.25.0\n",
        "  Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 76.4/76.4 KB 31.4 MB/s eta 0:00:00\n",
        "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (2.1.5)\n",
        "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (1.5.1)\n",
        "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (1.3.0)\n",
        "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (5.10.4)\n",
        "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (3.0.2)\n",
        "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (4.12.3)\n",
        "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.3.0)\n",
        "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.10.0)\n",
        "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
        "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter) (6.1.0)\n",
        "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (23.1.0)\n",
        "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (0.18.1)\n",
        "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (1.8.3)\n",
        "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (0.20.0)\n",
        "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (0.2.0)\n",
        "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter) (1.1.0)\n",
        "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from async-lru>=1.0.0->jupyterlab->jupyter) (4.12.2)\n",
        "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
        "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.16.0)\n",
        "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.4.0)\n",
        "Collecting httpcore==1.*\n",
        "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 77.9/77.9 KB 31.8 MB/s eta 0:00:00\n",
        "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2024.7.4)\n",
        "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
        "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.8)\n",
        "Collecting h11<0.15,>=0.13\n",
        "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 58.3/58.3 KB 25.0 MB/s eta 0:00:00\n",
        "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.3)\n",
        "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.19.1)\n",
        "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
        "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
        "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (1.2.2)\n",
        "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.4)\n",
        "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
        "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (4.2.2)\n",
        "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.5.3)\n",
        "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.10.0)\n",
        "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
        "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (7.7.0)\n",
        "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
        "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
        "Collecting json5>=0.9.0\n",
        "  Downloading json5-0.9.25-py3-none-any.whl (30 kB)\n",
        "Collecting babel>=2.10\n",
        "  Downloading babel-2.16.0-py3-none-any.whl (9.6 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9.6/9.6 MB 111.4 MB/s eta 0:00:0000:0100:01\n",
        "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.23.0)\n",
        "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.7->nbconvert->jupyter) (2.20.0)\n",
        "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.13)\n",
        "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.10/dist-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
        "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.6)\n",
        "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
        "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.35.1)\n",
        "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.20.0)\n",
        "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2023.12.1)\n",
        "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (24.2.0)\n",
        "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.1)\n",
        "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
        "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (0.1.4)\n",
        "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (2.0.7)\n",
        "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.3.2)\n",
        "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.2.2)\n",
        "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.17.0)\n",
        "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.4.1)\n",
        "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.0.1)\n",
        "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.3)\n",
        "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.22)\n",
        "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.0.0)\n",
        "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.5.1)\n",
        "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (24.8.0)\n",
        "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (20.11.0)\n",
        "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.3.0)\n",
        "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (1.3.0)\n",
        "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.9.0.20240821)\n",
        "Installing collected packages: tomli, json5, h11, babel, async-lru, httpcore, httpx, jupyter-console, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
        "Successfully installed async-lru-2.0.4 babel-2.16.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 json5-0.9.25 jupyter-1.1.1 jupyter-console-6.6.3 jupyter-lsp-2.2.5 jupyterlab-4.2.5 jupyterlab-server-2.27.3 tomli-2.0.1\n",
        "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.4.0\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Collecting torch==2.4.0\n",
        "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
        "     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 797.2/797.2 MB 1.5 MB/s eta 0:00:0000:0100:01\n",
        "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.0.2.54)\n",
        "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.1.4)\n",
        "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (10.3.2.106)\n",
        "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (11.4.5.107)\n",
        "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2.20.5)\n",
        "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (4.12.2)\n",
        "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
        "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
        "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.0.0)\n",
        "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
        "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
        "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.0.106)\n",
        "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (2024.6.1)\n",
        "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.15.4)\n",
        "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.105)\n",
        "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (3.3)\n",
        "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (12.1.3.1)\n",
        "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0) (1.13.2)\n",
        "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.20)\n",
        "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0) (2.1.5)\n",
        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
        "Installing collected packages: torch\n",
        "  Attempting uninstall: torch\n",
        "    Found existing installation: torch 2.4.1\n",
        "    Uninstalling torch-2.4.1:\n",
        "      Successfully uninstalled torch-2.4.1\n",
        "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "xformers 0.0.28.post1 requires torch==2.4.1, but you have torch 2.4.0 which is incompatible.\n",
        "Successfully installed torch-2.4.0\n",
        "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import FastLanguageModel from unsloth library\n",
        "from unsloth import FastLanguageModel\n",
        "# import SFTTrainer from trl\n",
        "from trl import SFTTrainer\n",
        "# import TrainingArguments from transformers\n",
        "from transformers import TrainingArguments\n",
        "# import evaluate library\n",
        "import evaluate\n",
        "# import locale library\n",
        "import locale\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\ud83e\udda5 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "import torch\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews_df= pd.read_csv('customer_reviews_dataset.csv')\n",
        "\n",
        "sample_reviews_df\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews_df['dialogue'] = 'customer: ' + sample_reviews_df['review_text'] + '\\n' + 'response: ' + sample_reviews_df['response']\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews_df['id'] = sample_reviews_df['customer_id']\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews_df = sample_reviews_df[['id', 'review_sentiment', 'dialogue', 'summary']]\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate positive and negative reviews\n",
        "positive_reviews =  sample_reviews_df[sample_reviews_df['review_sentiment'] == 'Positive']\n",
        "negative_reviews =  sample_reviews_df[sample_reviews_df['review_sentiment'] == 'Negative']\n",
        "\n",
        "# Sample 2 positive and 2 negative reviews for gold examples\n",
        "positive_gold_examples = positive_reviews.sample(2, random_state=40)\n",
        "negative_gold_examples = negative_reviews.sample(2, random_state=40)\n",
        "\n",
        "# Concatenate positive and negative gold examples\n",
        "sample_reviews_gold_examples_df =  pd.concat([positive_gold_examples, negative_gold_examples])\n",
        "\n",
        "# Create the training set by excluding gold examples\n",
        "sample_reviews_examples_df =  sample_reviews_df.drop(sample_reviews_gold_examples_df.index)\n",
        "\n",
        "# Print the shapes of the datasets\n",
        "print(\"Training Set Shape:\", sample_reviews_examples_df.shape)\n",
        "print(\"Gold Examples Shape:\", sample_reviews_gold_examples_df.shape)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Training Set Shape: (26, 4)\n",
        "Gold Examples Shape: (4, 4)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "    load_in_4bit=True # Use 4bit quantization to reduce memory usage.\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "==((====))==  Unsloth 2024.9.post3: Fast Llama patching. Transformers = 4.45.1.\n",
        "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
        "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
        "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
        " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.safetensors:   0%|          | 0.00/3.87G [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_config.json:   0%|          | 0.00/948 [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "LlamaForCausalLM(\n",
        "  (model): LlamaModel(\n",
        "    (embed_tokens): Embedding(32000, 4096)\n",
        "    (layers): ModuleList(\n",
        "      (0-31): 32 x LlamaDecoderLayer(\n",
        "        (self_attn): LlamaAttention(\n",
        "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (rotary_emb): LlamaRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): LlamaMLP(\n",
        "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
        "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
        "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "    (rotary_emb): LlamaRotaryEmbedding()\n",
        "  )\n",
        "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
        ")"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "LlamaTokenizerFast(name_or_path='unsloth/llama-2-7b-bnb-4bit', vocab_size=32000, model_max_length=4096, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
        "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
        "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
        "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
        "}"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0, # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=42,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Unsloth 2024.9.post3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"\"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the review.\"\n",
        "\n",
        "### Input:\n",
        "\"This product was great. I used it for a week and it performed excellently.\"\n",
        "\n",
        "### Response:\n",
        "\"The product performed well after a week of usage.\"\n",
        "\"\"\"\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_examples_with_seed(dataset, n=2, random_seed=None):\n",
        "    \"\"\"\n",
        "    Return two DataFrames with randomized examples of size 2n with two classes.\n",
        "    Create subsets of each class, choose random samples from the subsets,\n",
        "    merge and randomize the order of samples in the merged list.\n",
        "    Each run of this function creates a different random sample of examples\n",
        "    chosen from the training data.\n",
        "\n",
        "    Args:\n",
        "        dataset (DataFrame): A DataFrame with examples (text + label)\n",
        "        n (int): number of examples of each class to be selected\n",
        "        random_seed (int): seed for reproducibility (default is None)\n",
        "\n",
        "    Output:\n",
        "        few_shot_examples_df (DataFrame): A DataFrame with examples in random order\n",
        "        new_df (DataFrame): A new DataFrame excluding selected examples\n",
        "    \"\"\"\n",
        "\n",
        "    positive_reviews = (dataset.review_sentiment == 'Positive')\n",
        "    negative_reviews = (dataset.review_sentiment == 'Negative')\n",
        "    columns_to_select = ['id', 'review_sentiment' ,'dialogue','summary']\n",
        "\n",
        "    # Set a fixed random seed for reproducibility\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    positive_examples = dataset.loc[positive_reviews, columns_to_select].sample(n)\n",
        "    negative_examples = dataset.loc[negative_reviews, columns_to_select].sample(n)\n",
        "\n",
        "    few_shot_examples_df = pd.concat([positive_examples, negative_examples])\n",
        "    # sampling without replacement is equivalent to random shuffling\n",
        "    few_shot_examples_df = few_shot_examples_df.sample(2 * n, replace=False)\n",
        "\n",
        "    # Create a new DataFrame excluding selected examples\n",
        "    new_df = dataset.drop(index=few_shot_examples_df.index)\n",
        "\n",
        "    return few_shot_examples_df, new_df\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews_train_examples_df , sample_reviews_validation_examples_df =  create_examples_with_seed(\n",
        "    sample_reviews_examples_df, n=2, random_seed=42 \n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset = datasets.Dataset.from_pandas(sample_reviews_train_examples_df)\n",
        "validation_dataset = datasets.Dataset.from_pandas(sample_reviews_validation_examples_df)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_formatter(example, prompt_template):\n",
        "    instruction='Summarize the following dialogue'\n",
        "    dialogue=example[\"dialogue\"]\n",
        "    summary=example[\"summary\"]\n",
        "    \n",
        "    print(f\"Dialogue: {dialogue}\")\n",
        "    print(f\"Summary: {summary}\")\n",
        "\n",
        "    formatted_prompt = prompt_template.format(instruction, dialogue, summary)\n",
        "\n",
        "    return {'formatted_prompt': formatted_prompt}\n",
        "print(training_dataset)\n",
        "print(validation_dataset)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Dataset({\n",
        "    features: ['id', 'review_sentiment', 'dialogue', 'summary', '__index_level_0__'],\n",
        "    num_rows: 4\n",
        "})\n",
        "Dataset({\n",
        "    features: ['id', 'review_sentiment', 'dialogue', 'summary', '__index_level_0__'],\n",
        "    num_rows: 22\n",
        "})\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_training_dataset = training_dataset.map(\n",
        "    prompt_formatter,\n",
        "    fn_kwargs={'prompt_template': alpaca_prompt}\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Dialogue: customer: Originally bought it for my work, quite happy with it so far! Fast, reliable, easy to use and has a good webcam. Display is good and battery backup is also great. The keyboard is a joy to type on, gives me the old typewriter vibes! Quickly become my main laptop for everyday use, and I'm very satisfied with my purchase.\n",
        "response: Thank you for sharing your positive experience with our laptop! We're delighted to hear that it's serving you well for work and personal use, with its speed, reliability, ease of use, good webcam, and display quality. It's particularly pleasing to know that the battery life is meeting your needs and that you enjoy the typing experience reminiscent of a typewriter. We strive to create products that enhance everyday life, and your satisfaction with your purchase is a great affirmation of our efforts. If you ever have any questions or need further support, our customer support team is here to assist. Enjoy your laptop, and thank you for choosing our product!\n",
        "Summary: The customer is satisfied with their work laptop, describing it as fast, reliable, easy to use, with a good webcam, good display, and a long battery life. They appreciate the typewriter-like typing experience.\n",
        "Dialogue: customer: I bought this laptop quite recently, and I already have a problem with it. The laptop has a very bad screen, it is very dim and dull. The screen also has a lot of dead pixels, they are very visible and annoying. It also has a lot of glare, it is very hard to see in bright light. The battery capacity is good to be fair, and I'm happy with the quality of the keyboard. But because of the screen, I am very unhappy with this laptop.\n",
        "response: I'm sorry to hear about the issues you're experiencing with your laptop's screen. A dim, dull display with dead pixels and excessive glare can certainly detract from your overall experience. While it's good to know the battery capacity and keyboard quality meet your expectations, we understand that the screen quality is crucial. We take such feedback seriously as it helps us improve our products. Please reach out to our customer support to discuss this issue. We'd like to explore options for resolving this, which could include a repair or replacement of the screen. Your satisfaction is important to us, and we're committed to ensuring you have a positive experience with our laptop.\n",
        "Summary: The customer is unhappy with their recently purchased laptop due to its dim, dull screen, dead pixels, and excessive glare. The laptop's battery capacity and keyboard quality are satisfactory, but the screen quality is crucial for overall satisfaction.\n",
        "Dialogue: customer: Great for listening to music or podcasts. Good sound quality, battery life and charging speed. Quite comfortable too. Highly recommended!\n",
        "response: Thank you for your positive review of our headphones! We're delighted to hear that they meet your needs for listening to music and podcasts, with good sound quality, battery life, and charging speed. It's great to know that you find them comfortable as well. Your recommendation is highly appreciated, and we're thrilled that our product has been a good fit for you. If you ever have any questions or need assistance, our customer support team is always here to help. Enjoy your listening experience, and thank you for choosing our headphones!\n",
        "Summary: Customer praises headphones for good sound quality, battery life, and charging speed, recommending them for music and podcasts, expressing comfort and satisfaction.\n",
        "Dialogue: customer: I was very excited to get these headphones as I got the impression they have great noise cancellation and sound quality. However, I was very disappointed when I tried them. First of all, they're not comfortable to wear, and they end up hurting my head if I wear them for longer than 2 hours. Charging is a pain as well, it keeps getting interrupted for some reason. The sound was muffled and distorted, and the noise cancellation was barely noticeable. I tried to adjust the settings, but nothing worked. I contacted the seller, but they refused to refund or replace them. I feel like I wasted my money on these headphones.\n",
        "response: I'm truly sorry to hear about your disappointing experience with our headphones. Comfort, charging efficiency, sound quality, and effective noise cancellation are key aspects we strive to excel in, and it seems we fell short in your case. The discomfort, charging interruptions, and underwhelming audio performance are certainly not the standards we aim for. I apologize for the inconvenience and for the response you received from the seller. Please contact our customer support directly, referencing this conversation. We're committed to addressing your concerns and exploring all possible solutions, including a potential refund or replacement. Your satisfaction is crucial to us, and we want to ensure that you receive the quality product and service you expected.\n",
        "Summary: The customer was disappointed with their headphones, stating they were uncomfortable, causing head pain, and had distorted sound. The seller refused to refund or replace the headphones, causing the customer to feel they wasted their money. They are requesting a refund or replacement.\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_validation_dataset = validation_dataset.map(\n",
        "    prompt_formatter,\n",
        "    fn_kwargs={'prompt_template': alpaca_prompt}\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Map:   0%|          | 0/22 [00:00<?, ? examples/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Dialogue: customer: Awesome power bank, it charges my phone very fast and lasts for a long time. It is also very compact and lightweight, easy to carry around. It has a LED indicator that shows the battery level and a dual USB port that can charge two devices at the same time. It also has a low power mode that can charge small devices like earphones and smartwatches. I am very satisfied with this product.\n",
        "response: Thank you for your positive review of our power bank! We're thrilled to hear that its fast charging capability, long-lasting power, compact design, and dual USB ports meet your needs effectively. It's great to know that the LED indicator and low power mode for smaller devices add to your satisfaction. Your feedback is highly appreciated, and we're committed to maintaining this level of quality and convenience in our products. If you need any further assistance or information, please feel free to reach out to us.\n",
        "Summary: The user praises the power bank for its fast charging, long-lasting power, compact design, LED indicator, dual USB ports, and low power mode for small devices, expressing satisfaction with its quality and convenience.\n",
        "Dialogue: customer: I bought this phone mainly for its much-hyped camera, but I was very disappointed with the results. The pictures are blurry, grainy, and overexposed. The zoom is also very bad, it makes the pictures look pixelated and distorted. The night mode is also useless, it makes the pictures look dark and noisy. The video quality is also very poor, it lags and stutters. Battery is also not very good. Only good thing is display. I expected much better from a flagship phone.\n",
        "response: I'm truly sorry to hear that the camera performance of your phone did not meet your expectations, especially considering it's a key feature you were looking forward to. We take feedback about picture quality, zoom functionality, night mode, and video performance very seriously. I'll relay your concerns to our development team for further improvement. In the meantime, for the issues with blurriness, graininess, and battery performance, our customer service team would be glad to assist with troubleshooting or potential solutions. We're committed to ensuring your satisfaction and improving our flagship offerings. Please don't hesitate to reach out for further assistance.\n",
        "Summary: The user expressed disappointment with the phone's camera performance, stating blurry, grainy, overexposed pictures, poor zoom, distorted night mode, lags, stutters, and poor video quality. The phone's customer service team is working on improving these issues.\n",
        "Dialogue: customer: I love these headphones, they are amazing. The sound quality is very good, the bass is deep, and the treble is clear. The headphones are also very comfortable to wear, they fit snugly and do not fall off. The Bluetooth connection is also very stable, it does not drop or lag. The battery life is also very long, it lasts for more than 10 hours. Charging speed is also good . The headphones also have a built-in microphone that works well for calls and voice commands. I think these headphones are worth every penny.\n",
        "response: Thank you for your wonderful feedback on our headphones! We're delighted to hear that you're enjoying the sound quality, comfort, stable Bluetooth connection, long battery life, and the efficiency of the charging speed. It's great to know that the built-in microphone is serving you well for calls and voice commands. Your satisfaction is our goal, and we're thrilled that you find the headphones to be a valuable investment. If you ever need assistance or have any questions, our customer support team is always here to help. Thank you for choosing our product!\n",
        "Summary: The user praises the headphones for their excellent sound quality, comfort, stable Bluetooth connection, long battery life, and efficient charging speed. They also mention the built-in microphone for calls and voice commands.\n",
        "Dialogue: customer: This is a very bad power bank, it does not charge my phone properly. It takes a very long time to charge the power bank itself, and it drains very fast. It also does not charge my phone fully, it stops at around 80%. It also heats up very much, and sometimes it sparks and smokes. I think it is very dangerous and defective. I tried to return it, but the seller did not accept it. I feel cheated and scammed.\n",
        "response: I'm deeply concerned to hear about your experience with our power bank. Your safety is our top priority, and the issues you've described, particularly the overheating, sparking, and smoking, are serious. Please cease using the product immediately to avoid any risk. We want to address this as a matter of urgency. I apologize for the inconvenience caused and the dissatisfaction you've experienced with the charging performance and return process. Please contact our customer service directly, and we'll ensure that your case is prioritized for a resolution, including a replacement or refund as appropriate. Your trust in our products is crucial, and we're committed to restoring it.\n",
        "Summary: The customer reviews a poor power bank, stating it doesn't charge properly, drains quickly, stops at 80%, heats up, sparks, and smokes. The seller responds, apologizing for the inconvenience and urging the customer to contact customer service for resolution.\n",
        "Dialogue: customer: I bought this laptop a month ago, and it's already started giving me problems. The laptop overheats very quickly, and the fan makes a loud noise. The laptop also shuts down randomly, and sometimes it does not boot up at all. It's like a desktop PC to me, the battery capacity sucks and I can never work without a charger. The keyboard is also very faulty, some keys do not work properly, and some keys get stuck. The laptop is also very slow, it takes a long time to open applications and files.  Display is also below average. I contacted the customer service, but they were very unhelpful and rude. They refused to replace or repair the laptop, and they blamed me for the problems. I am very unhappy with this laptop and the service.\n",
        "response: I'm sorry to hear about your laptop issues and the poor service you received. Please contact us again, referencing this conversation. We'll prioritize resolving your problems, including overheating, keyboard, and performance issues, and discuss a suitable solution like repair or replacement. Your satisfaction is important, and we're committed to making this right.\n",
        "Summary: The customer reports issues with their laptop, including overheating, noise, random shutdowns, faulty keyboard, slow performance, and below-average display. The customer's customer service is unhelpful and rude, refusing to replace or repair the laptop.\n",
        "Dialogue: customer: This is a below average laptop. It has inconsistent battery life, and only lasts for about 4 hours. It also has poor performance and can't handle multiple tasks and applications. Its keyboard's keys are faulty, and one of them has nearly come off the device too. The display is the only good thing about it - good screen and good resolution. But overall, I'm not very happy with this purchase and wouldn't recommend it.\n",
        "response: I'm sorry to hear that your laptop isn't meeting your expectations, particularly with the battery life, performance, and keyboard issues. We appreciate your feedback on the display quality, but understand that overall satisfaction is key. We're committed to improving our products and service. For the battery and performance concerns, optimizing settings may help. Regarding the keyboard, we'd like to offer assistance or a possible repair. Please contact our customer support for guidance and potential solutions. We value your feedback and aim to make your experience with our products better.\n",
        "Summary: The customer reviews a below-average laptop with inconsistent battery life, poor performance, and faulty keyboard. They express disappointment and recommend contacting customer support for assistance or repair, aiming to improve overall satisfaction.\n",
        "Dialogue: customer: I bought these headphones a week ago, and they already stopped working. The battery is very poor, it does not last for more than an hour. The headphones also do not charge properly, they do not show the correct battery level and they do not turn on or off. The headphones are uncomfortable and also very faulty, they do not pair with my phone and they do not play any sound. I contacted the customer support, but they did not respond to me. I wasted my money on these headphones, they are useless.\n",
        "response: I apologize for the issues you're experiencing with the headphones and the lack of response from our customer support. It's concerning to hear about the battery life, charging problems, comfort, pairing issues, and sound malfunction. This is not the quality we strive for. Please reach out to us again and mention this interaction. We're committed to addressing these issues urgently, whether through troubleshooting, repair, or replacement. Your satisfaction is important, and we'll ensure a more responsive and effective service this time to resolve your concerns.\n",
        "Summary: The customer purchased poor-quality headphones, experiencing battery life issues, charging problems, discomfort, and sound malfunction. They contacted customer support but received no response. The company apologizes and promises to address these issues urgently.\n",
        "Dialogue: customer: These are the best headphones I have ever used. The sound quality is amazing, the bass is powerful, and the treble is crisp. The headphones are also very comfortable to wear, they are soft and adjustable. The Bluetooth connection is also very reliable, it does not break or lag. Charging  takes long though (4-5 hrs), but due to a long battery backup the situation's not as bad as it could've been. The headphones also have a handy button that lets me control the music and the calls. I highly recommend these headphones.\n",
        "response: Thank you for sharing your positive experience with our headphones! We're thrilled to hear that the sound quality, comfort, and Bluetooth connection are meeting your expectations. We appreciate your understanding regarding the charging time and are glad that the long battery life compensates for it. Your feedback about the convenience of the control button is also valued. We're continuously working to improve all aspects of our products, and your recommendation means a lot to us. If you ever have any questions or need assistance, please don't hesitate to reach out. Enjoy your music!\n",
        "Summary: The customer praises the headphones for their excellent sound quality, comfort, and reliable Bluetooth connection. They mention a 4-5 hour charging time but appreciate the long battery life and convenient control button. They highly recommend the headphones.\n",
        "Dialogue: customer: I hate this laptop, it is a nightmare. I bought it for the bright display it has (just about the only positive of the laptop), but the software is very poor - it has a lot of software problems, it is very slow and buggy. The laptop also has a lot of viruses and malware, they make the laptop crash and freeze and they drain battery very fast. The laptop also has a lot of pop-ups and ads, they are very annoying and distracting.They also provide quite a poor keyboard at this price range. I wish I never bought this laptop.\n",
        "response: I'm truly sorry to hear about the difficulties you're experiencing with your laptop. Dealing with software issues, viruses, malware, and intrusive pop-ups can be incredibly frustrating, especially when they impact the laptop's performance and battery life. We take such feedback seriously as it helps us improve. For immediate assistance, I recommend running a trusted antivirus program to address the malware and virus issues. Additionally, adjusting your browser settings can help reduce pop-ups and ads. For the software problems and keyboard concerns, please reach out to our customer support. We're here to help resolve these issues and improve your experience with our product. Your satisfaction is important to us, and we want to ensure you get the value you expected from your purchase.\n",
        "Summary: The customer reviews a laptop with poor software, slow performance, virus and malware issues, and annoying pop-ups. They suggest running a trusted antivirus program, adjusting browser settings, and reaching out to customer support for assistance in resolving these issues.\n",
        "Dialogue: customer: Recently tried a power bank sharing service - it's quite handy with easy pick-up and drop-off, but the charging speed varies. The power bank has good battery backup and useful with dual USB ports. Great for on-the-go charging, but consistency in performance could be improved.\n",
        "response: Thank you for your feedback on the power bank sharing service. It's great to hear that you find the pick-up and drop-off system convenient and the dual USB ports beneficial for on-the-go charging. Your point about the variation in charging speed is important to us. We aim for consistency in performance and will look into ways to enhance this aspect of our service. We appreciate your input as it helps us improve and provide a better experience for all users. If you have any more suggestions or need assistance, please feel free to reach out to us. Your satisfaction is our priority.\n",
        "Summary: The customer reviews a power bank sharing service, highlighting its convenience, battery backup, and dual USB ports. They mention varying charging speeds but suggest improvements in performance consistency.\n",
        "Dialogue: customer: I am very unhappy with this phone, it has a very bad battery life. The phone drains very fast, it does not last for more than a few hours.Display is below average. Camera is also not very good. The phone also takes a very long time to charge, it does not support fast charging. The phone also heats up very much, it is very uncomfortable to hold. The phone also has a poor customer service, they are very rude and unprofessional. They do not offer any solution or compensation for the battery issue. I do not recommend this phone at all.\n",
        "response: I'm truly sorry to hear about your dissatisfaction with the phone, especially regarding the battery life, display, camera quality, charging time, overheating, and your experience with our customer service. We take such feedback seriously and strive to improve in all these areas. The battery draining quickly and the heating issue are particularly concerning. We will address this with our technical team for improvements. Regarding customer service, we aim for professionalism and helpfulness, and I apologize that we did not meet this standard in your experience. Please reach out to us again, referencing this conversation, so we can escalate your concerns to a higher level for a more satisfactory resolution. We value your feedback and are committed to enhancing our products and services.\n",
        "Summary: The customer expresses dissatisfaction with the phone's battery life, display, camera quality, charging time, and customer service. The company apologizes for the negative feedback and promises to address the issues with their technical team. They encourage repeat communication for resolution.\n",
        "Dialogue: customer: I bought these headphones just 2 weeks ago, and I already regret it. The sound quality is quite poor, and charging takes too long. The headphones are really uncomfortable to wear, they hurt my ears and head. The headphones are also very tight and heavy, they make me sweat and itch. They also leak sound and disturb others, can you believe that? I do not like these headphones, they are a pain.\n",
        "response: I'm sorry to hear about your negative experience with our headphones. Issues like poor sound quality, long charging times, discomfort, and sound leakage are not what we aim for. We appreciate your feedback as it helps us improve our products. For immediate relief, you might try adjusting the fit, if adjustable, to alleviate discomfort. We're committed to customer satisfaction and would like to make this right for you. Please contact our customer support, mentioning your concerns and this conversation. We'll work on a solution, whether it's troubleshooting, a possible exchange, or a different resolution. Your comfort and satisfaction are our top priorities.\n",
        "Summary: The customer is unhappy with their headphones, stating poor sound quality, long charging times, discomfort, and sound leakage. The company apologizes and offers to address these issues through adjustments and customer support.\n",
        "Dialogue: customer: This is a great laptop, I am very happy with it. Great battery life, it lasts for about 8 hours. It has great performance, it can handle multiple tasks and applications. Good storage capacity, it can store a lot of files and data. The laptop also has a great screen, it has a good resolution and viewing angle. It also has a great design, it's sturdy and durable and the keyboard's keys are good and strong. Overall I found it perfect, with basically no flaws at all. Great buy!\n",
        "response: Thank you for your positive feedback on our laptop! We're thrilled to hear that the battery life, performance, storage capacity, screen quality, design, and keyboard are meeting your expectations and providing a great experience. It's wonderful to know that you're satisfied with your purchase and find the laptop to be flawless for your needs. We strive to deliver high-quality products, and your satisfaction is a strong affirmation of our efforts. If you ever have any questions or need assistance, our customer support team is always here to help. Enjoy your laptop, and thank you for choosing our product!\n",
        "Summary: The customer is satisfied with their laptop, stating it has excellent battery life, performance, storage capacity, screen quality, design, and keyboard. They appreciate the high-quality product and express gratitude for the customer support team's assistance.\n",
        "Dialogue: customer: This is a good power bank, it does what it says. It charges my phone and other devices quickly and safely. It is also easy to use and carry. It has a simple design and a good capacity. It also has a smart protection system that prevents overcharging and overheating. It is a good product for the price.\n",
        "response: Thank you for sharing your positive experience with our power bank. We're glad to hear that it meets your expectations in terms of charging efficiency, ease of use, portability, and design. It's particularly encouraging to know that the smart protection system for preventing overcharging and overheating is adding value to your experience. We aim to offer quality products at a fair price, and your feedback confirms our commitment to this goal. If you have any further questions or need assistance, please feel free to contact our customer support. Enjoy your device, and thank you for choosing our product!\n",
        "Summary: The customer praises the power bank for its efficient charging, portability, and smart protection system. The company acknowledges the positive feedback and offers customer support for further assistance.\n",
        "Dialogue: customer: Good phone, I am satisfied with it. The phone has a good design, it is slim and light. The screen is also big and bright, it has a high resolution and a smooth refresh rate. The phone is also fast and smooth, it has a powerful processor and a large memory. The phone also has a good camera, it takes good pictures and videos. The battery life is also good, it lasts for a whole day. The phone also has a lot of features and functions, like wireless charging, fingerprint scanner, face recognition, and more. I think this phone is a good buy.\n",
        "response: Thank you for your positive review of the phone! We're delighted to hear that the design, screen quality, performance, camera, battery life, and additional features like wireless charging, fingerprint scanner, and face recognition meet your expectations and contribute to a satisfying experience. We strive to combine functionality, performance, and user-friendly features in our products, and it's rewarding to know we've achieved this with your phone. If you ever have any questions or need assistance, our customer support team is always here to help. Enjoy your phone, and thank you for choosing our product!\n",
        "Summary: The customer is satisfied with their phone, praised for its slim design, large screen, high resolution, powerful processor, large memory, camera, and battery life. The phone also offers features like wireless charging, fingerprint scanner, and face recognition.\n",
        "Dialogue: customer: The headphones work well. The sound quality is good, the bass is decent, and the treble is clear. The headphones are also comfortable to wear, they are soft and adjustable. The Bluetooth connection is also reliable, it does not break or lag. Charging is also good and battery backup is decent as well. Only problem i feel is that  headphones could  have been more handy with smaller adjustable buttons. Overall, I like these headphones, they are good.\n",
        "response: Thank you for your review of our headphones. It's great to hear that you're satisfied with the sound quality, comfort, Bluetooth connection, charging, and battery life. Your feedback about the size of the adjustable buttons is valuable to us. We constantly aim to improve our products based on customer input, and suggestions like yours are crucial for future enhancements. We appreciate your overall positive experience and are glad you like the headphones. If you have any further suggestions or need assistance, our customer support team is here to help. Enjoy your listening experience, and thank you for choosing our product!\n",
        "Summary: The customer praises the headphones for their sound quality, comfort, and reliable Bluetooth connection. They mention a minor issue with the size of the adjustable buttons, but overall, they appreciate the positive feedback.\n",
        "Dialogue: customer: Bought this phone because I wanted to upgrade from my old iPhone. I liked the design and the features of this phone, but I was very disappointed with the battery life. The phone would drain very quickly, even when I was not using it much. I had to charge it several times a day, which was very annoying.I think only plus point to this phone is camera. Even the display is quite average at this price. I contacted customer care, but they said it was normal and there was nothing they could do. I regret buying this phone.\n",
        "response: I'm sorry to hear about your disappointment with the battery life of your new phone, especially considering your satisfaction with its design and camera. It's concerning that the phone drains quickly and requires multiple charges a day, which is indeed inconvenient. While it's standard for battery life to vary based on usage, your experience seems unusual. We take such feedback seriously and continuously work on improving our products. I apologize for the response you received from customer care. I encourage you to reach out again, mentioning this conversation for further review of your case. We're committed to finding a more satisfactory solution for you, whether that's through troubleshooting, a repair, or another appropriate resolution. Your satisfaction is important to us.\n",
        "Summary: The customer purchased a phone to upgrade from their iPhone, but was disappointed with its battery life, draining quickly and requiring multiple daily charges. Despite contacting customer care, they were told it was normal and could not fix it.\n",
        "Dialogue: customer: Amazing phone. It has a beautiful design and a great performance. The camera is stunning and the face ID is very convenient. The phone is also very fast and smooth. The battery life is decent and the wireless charging is a nice feature. I am very happy with this phone. Only display size could have been big. Overall , I would recommend it to anyone who wants a premium smartphone.\n",
        "response: Thank you for your enthusiastic review of the phone! We're delighted to hear that its design, performance, camera quality, face ID, smooth operation, battery life, and wireless charging features are to your liking and meet your expectations. It's great to know you're enjoying the overall experience. We appreciate your feedback on the display size and will consider it for future improvements. Your recommendation is highly valued, and we're glad that you consider the phone a worthy choice for those seeking a premium smartphone. If you have any further questions or need support, our customer support team is always here to assist. Enjoy your phone, and thank you for your support!\n",
        "Summary: The customer praises the phone's design, performance, camera quality, face ID, smooth operation, decent battery life, and wireless charging. They recommend it for premium smartphone users, despite a smaller display size.\n",
        "Dialogue: customer: I love this power bank - it's amazing. It can charge my phone several times and it is very compact and portable. It also has a fast charging feature, which is very convenient. The power bank is durable and has a sleek design. I am very satisfied with this product and I would highly recommend it.\n",
        "response: Thank you for your enthusiastic feedback on our power bank! We're thrilled to hear that its capacity, compactness, portability, fast charging feature, durability, and sleek design are all contributing to a great experience for you. Your satisfaction is a top priority for us, and we're delighted to know that our product meets your expectations. Your recommendation is greatly appreciated and helps others make informed decisions. If you have any further questions or need support, our customer service team is always ready to assist. Enjoy your power bank, and thank you for your support!\n",
        "Summary: The customer praises the power bank's capacity, portability, fast charging feature, durability, and sleek design. They highly recommend it and appreciate the customer's enthusiastic feedback, ensuring a positive experience.\n",
        "Dialogue: customer: I bought these headphones as a wireless option for my commute, but I ended up returning them in one week - they hurt my ears and kept slipping from a comfortable position without staying stable in one place. The charging was buggy and I needed to keep reconnecting the cable. I would also occasionally get static noise and interference, and I eventually felt they weren't worth what I'd paid for them.\n",
        "response: I'm sorry to hear about your unsatisfactory experience with our headphones. Comfort, stable fit, reliable charging, and clear sound quality are crucial for a good wireless headphone experience, especially during commutes. The issues you encountered with discomfort, fit, charging, and static noise are not the standards we aim to uphold. I apologize for any inconvenience this has caused you. We appreciate your feedback as it's essential for us to improve our products. If you have any additional feedback or need further assistance in the future, please don't hesitate to contact our customer support. We're committed to ensuring our customers have a positive experience with our products.\n",
        "Summary: The customer returned their wireless headphones due to discomfort, slipping, and unstable positioning. The company apologized and promised to improve their products to ensure a positive experience for customers, including addressing issues with charging and static noise.\n",
        "Dialogue: customer: Good backup for my phone & other devices. Small and light, with two USB-C and two USB-A ports, and a nice LED indicator showing the charge remaining inside it. Good battery capacity. Design is pretty decent, and a fast charging speed makes it even better. Overall, a good purchase for the price I paid.\n",
        "response: Thank you for your positive feedback on our power bank! We're pleased to hear that it effectively serves as a backup for your phone and other devices. The compact size, lightness, combination of USB-C and USB-A ports, and the LED indicator for remaining charge are features we're proud of. It's great to know that the battery capacity, design, and fast charging speed meet your expectations and add value to your experience. We aim to provide quality products at a fair price, and your satisfaction reaffirms our commitment. If you have any further questions or need assistance, our customer support team is always here to help. Enjoy your power bank, and thank you for choosing our product!\n",
        "Summary: The customer praises the compact, lightweight power bank for its battery capacity, design, and fast charging speed. They appreciate the LED indicator for remaining charge and the customer's satisfaction with the product.\n",
        "Dialogue: customer: I thought this would be the laptop I could use for my research and studies. But I honestly regret my purchase now. Within a month, the laptop has become slow and laggy, and it struggles to handle multiple browser tabs or programs open at the same time. The keyboard sucks - I hate typing on it. It sometimes also has an overheating problem when I keep it switched on for a long time. Battery drains quite fast. Other than the screen display, it has nothing good. It's been a frustrating buy and I don't recommend it at all.\n",
        "response: I'm sorry to hear about the issues you're experiencing with your laptop, especially since it's affecting your research and studies. The problems with speed, lag, keyboard discomfort, overheating, and fast battery drain are certainly not what we want our customers to experience. We take your feedback seriously, as it helps us identify areas for improvement. Please reach out to our customer support team to discuss these issues. We're committed to finding a solution for you, whether it's through troubleshooting, repairs, or discussing other options. Your satisfaction is important to us, and we want to ensure that your laptop meets your needs for your important work.\n",
        "Summary: The customer regrets their purchase of a slow, laggy laptop for research and studies. The laptop struggles with multiple programs, keyboard discomfort, overheating, and fast battery drain. The customer's feedback is taken seriously and addressed through customer support.\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_training_dataset,\n",
        "    eval_dataset=formatted_validation_dataset,\n",
        "    dataset_text_field = \"formatted_prompt\",\n",
        "    max_seq_length=2048,\n",
        "    dataset_num_proc=2,\n",
        "    packing=False, # Increases efficiency for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=50,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=42,\n",
        "        output_dir=\"outputs\"\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Map (num_proc=2):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Map (num_proc=2):   0%|          | 0/22 [00:00<?, ? examples/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Detected kernel version 4.14.350, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
        "max_steps is given, it will override any value given in num_train_epochs\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "GPU = Tesla T4. Max memory = 14.581 GB.\n",
        "3.826 GB of memory reserved.\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
        "   \\\\   /|    Num examples = 4 | Num Epochs = 50\n",
        "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
        "\\        /    Total batch size = 8 | Total steps = 50\n",
        " \"-____-\"     Number of trainable parameters = 39,976,960\n",
        "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
        "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
        "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
        "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "TrainOutput(global_step=50, training_loss=0.1633661100921745, metrics={'train_runtime': 74.596, 'train_samples_per_second': 5.362, 'train_steps_per_second': 0.67, 'total_flos': 486583866163200.0, 'train_loss': 0.1633661100921745, 'epoch': 50.0})"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.Dataset.from_pandas(sample_reviews_gold_examples_df)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[0]\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "{'id': 'CID041',\n",
        " 'review_sentiment': 'Positive',\n",
        " 'dialogue': \"customer: I bought this laptop for my son who is studying engineering. He is very happy with it. It has a good battery life, fast performance, and a sleek design. The keyboard is comfortable and the screen is bright. The laptop came with a one-year warranty and a free antivirus software. I think it is a great value for money.\\nresponse: It's fantastic to hear that the laptop you purchased for your son has met his needs and expectations, especially in his engineering studies. A good battery life, fast performance, and sleek design are essential for a student's productivity. The comfortable keyboard and bright screen further enhance the usability of the laptop. If you ever encounter any issues or have questions about the laptop, please feel free to reach out for support. We're here to ensure that your experience continues to be positive. Thank you for choosing our product and taking the time to share your satisfaction!\",\n",
        " 'summary': 'The user purchased a laptop for their son, who is studying engineering. They are satisfied with its battery life, fast performance, sleek design, comfortable keyboard, and bright screen, and its one-year warranty.',\n",
        " '__index_level_0__': 0}"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Summarize the following dialogue\"\n",
        "test_dialogue = test_dataset[0]['dialogue']\n",
        "test_summary = test_dataset[0]['summary']\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "PeftModelForCausalLM(\n",
        "  (base_model): LoraModel(\n",
        "    (model): LlamaForCausalLM(\n",
        "      (model): LlamaModel(\n",
        "        (embed_tokens): Embedding(32000, 4096)\n",
        "        (layers): ModuleList(\n",
        "          (0-31): 32 x LlamaDecoderLayer(\n",
        "            (self_attn): LlamaAttention(\n",
        "              (q_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (k_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (v_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (o_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (rotary_emb): LlamaRotaryEmbedding()\n",
        "            )\n",
        "            (mlp): LlamaMLP(\n",
        "              (gate_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (up_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (down_proj): lora.Linear4bit(\n",
        "                (base_layer): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
        "                (lora_dropout): ModuleDict(\n",
        "                  (default): Identity()\n",
        "                )\n",
        "                (lora_A): ModuleDict(\n",
        "                  (default): Linear(in_features=11008, out_features=16, bias=False)\n",
        "                )\n",
        "                (lora_B): ModuleDict(\n",
        "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
        "                )\n",
        "                (lora_embedding_A): ParameterDict()\n",
        "                (lora_embedding_B): ParameterDict()\n",
        "                (lora_magnitude_vector): ModuleDict()\n",
        "              )\n",
        "              (act_fn): SiLU()\n",
        "            )\n",
        "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "          )\n",
        "        )\n",
        "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "        (rotary_emb): LlamaRotaryEmbedding()\n",
        "      )\n",
        "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
        "    )\n",
        "  )\n",
        ")"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        instruction,\n",
        "        test_dialogue,\n",
        "        \"\", # leave output blank for generation\n",
        "    )\n",
        "], return_tensors=\"pt\").to(\"cuda\")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    do_sample=True,\n",
        "    temperature=0.2\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.batch_decode(outputs)[0])\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "<s> \"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the review.\"\n",
        "\n",
        "### Input:\n",
        "\"This product was great. I used it for a week and it performed excellently.\"\n",
        "\n",
        "### Response:\n",
        "\"The product performed well after a week of usage.\"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the review.\"\n",
        "\n",
        "### Input:\n",
        "\"This product was great. I used it for a week and it performed excellently.\"\n",
        "\n",
        "### Response:\n",
        "\"The product performed well after a week of usage.\"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the review.\"\n",
        "\n",
        "### Input:\n",
        "\"This product was great. I used it for a week and it performed excellently.\"\n",
        "\n",
        "### Response:\n",
        "\"The product performed well after a week of usage.\"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_summary\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "'The user purchased a laptop for their son, who is studying engineering. They are satisfied with its battery life, fast performance, sleek design, comfortable keyboard, and bright screen, and its one-year warranty.'"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "\n",
        "locale.getpreferredencoding = getpreferredencoding\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model_name = \"dialogue-summarizer-llama\"\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model using save_pretrained function from model\n",
        "model.save_pretrained(lora_model_name)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh {lora_model_name}\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "total 153M\n",
        "-rw-r--r-- 1 root root 5.0K Sep 28 02:09 README.md\n",
        "-rw-r--r-- 1 root root  727 Sep 28 02:09 adapter_config.json\n",
        "-rw-r--r-- 1 root root 153M Sep 28 02:09 adapter_model.safetensors\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r {lora_model_name} /data/\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cp: 'dialogue-summarizer-llama' and '/data/dialogue-summarizer-llama' are the same file\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name= \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "    load_in_4bit=True # Use 4bit quantization to reduce memory usage.\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "==((====))==  Unsloth 2024.9.post3: Fast Llama patching. Transformers = 4.45.1.\n",
        "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
        "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
        "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
        " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(baseline_model)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "LlamaForCausalLM(\n",
        "  (model): LlamaModel(\n",
        "    (embed_tokens): Embedding(32000, 4096)\n",
        "    (layers): ModuleList(\n",
        "      (0-31): 32 x LlamaDecoderLayer(\n",
        "        (self_attn): LlamaAttention(\n",
        "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (rotary_emb): LlamaRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): LlamaMLP(\n",
        "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
        "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
        "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "    (rotary_emb): LlamaRotaryEmbedding()\n",
        "  )\n",
        "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
        ")"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.Dataset.from_pandas(sample_reviews_gold_examples_df)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Summarize the following dialogue\"\n",
        "test_dialogue = test_dataset[0]['dialogue']\n",
        "test_summary = test_dataset[0]['summary']\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer(\n",
        "    alpaca_prompt.format(\n",
        "        instruction,\n",
        "        test_dialogue,\n",
        "        \"\"\n",
        "    ), return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "output = baseline_model.generate(\n",
        "    **input,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    do_sample=True,\n",
        "    temperature=0.2\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output[0]))\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "<s> \"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the review.\"\n",
        "\n",
        "### Input:\n",
        "\"This product was great. I used it for a week and it performed excellently.\"\n",
        "\n",
        "### Response:\n",
        "\"The product performed well after a week of usage.\"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the review.\"\n",
        "\n",
        "### Input:\n",
        "\"This product was great. I used it for a week and it performed excellently.\"\n",
        "\n",
        "### Response:\n",
        "\"The product performed well after a week of usage.\"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the review.\"\n",
        "\n",
        "### Input:\n",
        "\"This product was great. I used it for a week and it performed excellently.\"\n",
        "\n",
        "### Response:\n",
        "\"The product performed well after a week of usage.\"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Summarize the following dialogue\"\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_dialogues = [sample['dialogue'] for sample in test_dataset]\n",
        "test_summaries = [sample['summary'] for sample in test_dataset]\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_summary_from_string(input_string):\n",
        "    try:\n",
        "        # Assuming the response is between ### Response: and </s>\n",
        "        summary_start = input_string.rfind('### Response:\\n') + 14 # number of characters in '### Response:\\n'\n",
        "        summary_end = input_string.rfind('</s>')\n",
        "        summary_str = input_string[summary_start:summary_end]\n",
        "\n",
        "        return summary_str\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding string: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_summaries = []\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for sample_dialogue in test_dialogues:\n",
        "  input = tokenizer(\n",
        "    alpaca_prompt.format(\n",
        "        instruction,\n",
        "        sample_dialogue,\n",
        "        \"\"\n",
        "    ), return_tensors=\"pt\"\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  outputs = baseline_model.generate(\n",
        "    **input,\n",
        "    max_new_tokens=256,\n",
        "    use_cache=True\n",
        "  )\n",
        "\n",
        "  predicted_summary = tokenizer.decode(outputs[0])\n",
        "\n",
        "  output_str = extract_summary_from_string(predicted_summary)\n",
        "\n",
        "  predicted_summaries.append(output_str)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "bert_scorer = evaluate.load(\"bertscore\")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide Prediction Summaries and Test Summaries as input\n",
        "score = bert_scorer.compute(\n",
        "    predictions=predicted_summaries,\n",
        "    references=test_summaries,\n",
        "    lang=\"en\",\n",
        "    rescale_with_baseline=True\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
        "  warnings.warn(\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#  Calculate Average Bert Score . Average Bert Score is sum of f1 score divided by number of samples\n",
        "average_bert_score = sum(score['f1']) / len(score['f1'])\n",
        "print()\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model_name = \"dialogue-summarizer-llama\"\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "\n",
        "locale.getpreferredencoding = getpreferredencoding\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /data/{lora_model_name} .\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cp: '/data/dialogue-summarizer-llama' and './dialogue-summarizer-llama' are the same file\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name= \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "==((====))==  Unsloth 2024.9.post3: Fast Llama patching. Transformers = 4.45.1.\n",
        "   \\\\   /|    GPU: Tesla T4. Max memory: 14.581 GB. Platform = Linux.\n",
        "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
        "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
        " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "LlamaForCausalLM(\n",
        "  (model): LlamaModel(\n",
        "    (embed_tokens): Embedding(32000, 4096)\n",
        "    (layers): ModuleList(\n",
        "      (0-31): 32 x LlamaDecoderLayer(\n",
        "        (self_attn): LlamaAttention(\n",
        "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
        "          (rotary_emb): LlamaRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): LlamaMLP(\n",
        "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
        "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
        "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "      )\n",
        "    )\n",
        "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
        "    (rotary_emb): LlamaRotaryEmbedding()\n",
        "  )\n",
        "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
        ")"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.Dataset.from_pandas(sample_reviews_gold_examples_df)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Summarize the following dialogue\"\n",
        "test_dialogue = test_dataset[0]['dialogue']\n",
        "test_summary = test_dataset[0]['summary']\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer(\n",
        "    alpaca_prompt.format(\n",
        "        instruction,\n",
        "        test_dialogue,\n",
        "        \"\"\n",
        "    ), return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(\n",
        "    **input,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    do_sample=True,\n",
        "    temperature=0.2\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output[0]))\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "<s> \"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the review.\"\n",
        "\n",
        "### Input:\n",
        "\"This product was great. I used it for a week and it performed excellently.\"\n",
        "\n",
        "### Response:\n",
        "\"The product performed well after a week of usage.\"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the review.\"\n",
        "\n",
        "### Input:\n",
        "\"This product was great. I used it for a week and it performed excellently.\"\n",
        "\n",
        "### Response:\n",
        "\"The product performed well after a week of usage.\"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the review.\"\n",
        "\n",
        "### Input:\n",
        "\"This product was great. I used it for a week and it performed excellently.\"\n",
        "\n",
        "### Response:\n",
        "\"The product performed well after a week of usage.\"\n",
        "\n",
        "### Instruction:\n",
        "\"Summarize the\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"Summarize the following dialogue\"\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# test_size = 4\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_dialogues = [sample['dialogue'] for sample in test_dataset]\n",
        "test_summaries = [sample['summary'] for sample in test_dataset]\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_summary_from_string(input_string):\n",
        "    try:\n",
        "        # Assuming the response is between ### Response: and </s>\n",
        "        summary_start = input_string.rfind('### Response:\\n') + 14 # number of characters in '### Response:\\n'\n",
        "        summary_end = input_string.rfind('</s>')\n",
        "        summary_str = input_string[summary_start:summary_end]\n",
        "\n",
        "        return summary_str\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding string: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_summaries = []\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for sample_dialogue in test_dialogues:\n",
        "  input = tokenizer(\n",
        "    alpaca_prompt.format(\n",
        "        instruction,\n",
        "        sample_dialogue,\n",
        "        \"\"\n",
        "    ), return_tensors=\"pt\"\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  outputs = model.generate(\n",
        "    **input,\n",
        "    max_new_tokens=256,\n",
        "    use_cache=True\n",
        "  )\n",
        "\n",
        "  predicted_summary = tokenizer.decode(outputs[0])\n",
        "\n",
        "  output_str = extract_summary_from_string(predicted_summary)\n",
        "\n",
        "  predicted_summaries.append(output_str)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Input prediction summaries and test summaries in bert scorer\n",
        "score = bert_scorer.compute(\n",
        "    predictions= predicted_summaries,\n",
        "    references= test_summaries,\n",
        "    lang=\"en\",\n",
        "    rescale_with_baseline=True\n",
        ")\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#  Calculate Average Bert Score . Average Bert Score is sum of f1 score divided by number of samples\n",
        "average_bert_score = sum(score['f1']) / len(score['f1'])\n",
        "print(average_bert_score)\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "-0.0034838546998798847\n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        " \n"
      ],
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 4
}