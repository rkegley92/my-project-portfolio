{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMSyYyAad70DfpeT3XtCG8E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"raBkyXnK3URp"},"outputs":[],"source":["\n","# Cell 1: Install/Import Dependencies\n","\n","# If you need any additional libraries (for example xgboost or others), install here with pip:\n","# !pip install xgboost\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","# TensorFlow / Keras\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","print(tf.__version__)"]},{"cell_type":"code","source":["\n","# Cell 2: Load CSV & Basic Preprocessing\n","\n","def load_and_preprocess_data(filepath: str,\n","                            date_col: str = 'date',\n","                            index_col: str = 'date',\n","                            interp_method: str = 'linear'\n","                            ) -> pd.DataFrame:\n","    \"\"\"\n","    Loads a CSV file containing hydrologic data, parses dates, sets index,\n","    and handles missing values by interpolation.\n","    \"\"\"\n","    # Read CSV, parse dates\n","    df = pd.read_csv(filepath, parse_dates=[date_col], index_col=index_col)\n","    # Interpolate missing values\n","    df.interpolate(method=interp_method, inplace=True)\n","    return df\n","\n","# Example usage: adapt the filename/columns to your data\n","csv_file = 'your_data.csv'  # Replace with your file path\n","df = load_and_preprocess_data(\n","    filepath=csv_file,\n","    date_col='date',\n","    index_col='date',\n","    interp_method='linear'\n",")\n","\n","# Quick check on the data\n","print(df.head())\n","print(df.isna().sum())"],"metadata":{"id":"CME2Sgzu3YFX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Cell 3: Scaling & Sequence Creation\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","def scale_data(df: pd.DataFrame):\n","    \"\"\"\n","    Fits a MinMaxScaler on the DataFrame and returns both the scaler and the scaled DataFrame.\n","    \"\"\"\n","    scaler = MinMaxScaler()\n","    df_scaled = pd.DataFrame(\n","        scaler.fit_transform(df),\n","        index=df.index,\n","        columns=df.columns\n","    )\n","    return scaler, df_scaled\n","\n","def create_sequences(data: np.ndarray, seq_length: int, target_index: int):\n","    \"\"\"\n","    Converts a 2D array [samples, features] into sequences of shape\n","    [samples, seq_length, features] for LSTM. The target is the value\n","    at `target_index` after `seq_length` steps.\n","    \"\"\"\n","    X, y = [], []\n","    for i in range(len(data) - seq_length):\n","        X_seq = data[i : i + seq_length, :]\n","        y_val = data[i + seq_length, target_index]\n","        X.append(X_seq)\n","        y.append(y_val)\n","    return np.array(X), np.array(y)\n","\n","# 1) Scale the Data\n","scaler, df_scaled = scale_data(df)\n","\n","# 2) Define features and target columns\n","feature_cols = ['precipitation', 'temperature', 'flow']  # Example columns\n","target_col = 'flow'  # We want to predict 'flow'\n","\n","seq_len = 14  # e.g., 14 days history\n","data_array = df_scaled[feature_cols].values\n","target_index = feature_cols.index(target_col)\n","\n","X_all, y_all = create_sequences(data_array, seq_length=seq_len, target_index=target_index)\n","\n","print(\"Shape of X_all:\", X_all.shape)\n","print(\"Shape of y_all:\", y_all.shape)"],"metadata":{"id":"nYRGNdTl3c4P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Cell 4: Train/Val/Test Split (Time Series)\n","\n","n_samples = X_all.shape[0]\n","train_size = int(n_samples * 0.70)\n","val_size   = int(n_samples * 0.15)\n","\n","X_train, y_train = X_all[:train_size], y_all[:train_size]\n","X_val,   y_val   = X_all[train_size : train_size + val_size], y_all[train_size : train_size + val_size]\n","X_test,  y_test  = X_all[train_size + val_size:], y_all[train_size + val_size:]\n","\n","print(f\\\"Train samples: {X_train.shape[0]}\\\")\n","print(f\\\"Val samples:   {X_val.shape[0]}\\\")\n","print(f\\\"Test samples:  {X_test.shape[0]}\\\")"],"metadata":{"id":"7F_egEnL3gt3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Cell 5: Model Definition\n","\n","def build_lstm_model(seq_length: int,\n","                     num_features: int,\n","                     lstm_units_1: int = 64,\n","                     lstm_units_2: int = 32,\n","                     dropout_rate: float = 0.2) -> Sequential:\n","    \"\"\"\n","    Builds and compiles an LSTM-based neural network.\n","    \"\"\"\n","    model = Sequential()\n","    # First LSTM layer (return_sequences=True to feed the second LSTM)\n","    model.add(LSTM(lstm_units_1, activation='relu', return_sequences=True,\n","                   input_shape=(seq_length, num_features)))\n","    model.add(Dropout(dropout_rate))\n","\n","    # Second LSTM layer\n","    model.add(LSTM(lstm_units_2, activation='relu'))\n","    model.add(Dropout(dropout_rate))\n","\n","    # Output layer for regression\n","    model.add(Dense(1))\n","\n","    # Compile the model\n","    model.compile(\n","        optimizer='adam',\n","        loss='mse'  # For regression tasks, MSE is common\n","    )\n","    return model\n","\n","model = build_lstm_model(\n","    seq_length=seq_len,\n","    num_features=len(feature_cols),\n","    lstm_units_1=64,\n","    lstm_units_2=32,\n","    dropout_rate=0.2\n",")\n","\n","model.summary()"],"metadata":{"id":"8VBycD--3kFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Cell 6: Model Training\n","\n","def train_model(model,\n","                X_train, y_train,\n","                X_val, y_val,\n","                epochs: int = 50,\n","                batch_size: int = 32,\n","                patience: int = 10,\n","                model_save_path: str = 'best_model.h5'):\n","    \"\"\"\n","    Trains the LSTM model using early stopping and saves the best model via ModelCheckpoint.\n","    \"\"\"\n","    early_stop = EarlyStopping(\n","        monitor='val_loss',\n","        patience=patience,\n","        restore_best_weights=True,\n","        verbose=1\n","    )\n","    checkpoint = ModelCheckpoint(\n","        filepath=model_save_path,\n","        monitor='val_loss',\n","        save_best_only=True,\n","        verbose=1\n","    )\n","\n","    history = model.fit(\n","        X_train, y_train,\n","        validation_data=(X_val, y_val),\n","        epochs=epochs,\n","        batch_size=batch_size,\n","        callbacks=[early_stop, checkpoint],\n","        verbose=1\n","    )\n","\n","    # Optionally load the best saved weights\n","    model.load_weights(model_save_path)\n","    return model, history\n","\n","model_save_path = 'best_model.h5'  # Name of the file to save the best model\n","trained_model, history = train_model(\n","    model,\n","    X_train, y_train,\n","    X_val, y_val,\n","    epochs=50,\n","    batch_size=32,\n","    patience=10,\n","    model_save_path=model_save_path\n",")"],"metadata":{"id":"Sj_5TCcQ3m88"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Cell 7: Evaluation & Visualization\n","\n","def evaluate_model(model,\n","                   X_test: np.ndarray,\n","                   y_test: np.ndarray,\n","                   label: str = 'Test'):\n","    \"\"\"\n","    Evaluates the model using RMSE, MAE, and Nash-Sutcliffe Efficiency (NSE).\n","    \"\"\"\n","    y_pred = model.predict(X_test).flatten()\n","\n","    rmse_val = np.sqrt(mean_squared_error(y_test, y_pred))\n","    mae_val  = mean_absolute_error(y_test, y_pred)\n","\n","    # Nashâ€“Sutcliffe Efficiency\n","    numerator = np.sum((y_test - y_pred)**2)\n","    denominator = np.sum((y_test - np.mean(y_test))**2)\n","    nse_val = 1 - (numerator / denominator)\n","\n","    print(f\\\"[{label} Evaluation]\\\")\n","    print(f\\\"RMSE: {rmse_val:.4f}\\\")\n","    print(f\\\"MAE:  {mae_val:.4f}\\\")\n","    print(f\\\"NSE:  {nse_val:.4f}\\\")\n","    return y_pred\n","\n","# Evaluate on the test set\n","y_pred_test = evaluate_model(trained_model, X_test, y_test, label='Test')\n","\n","# Visualize training/validation loss history\n","plt.figure(figsize=(8,4))\n","plt.plot(history.history['loss'], label='Train Loss')\n","plt.plot(history.history['val_loss'], label='Val Loss')\n","plt.title('Training History')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss (MSE)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","\n","# Plot actual vs predicted in the test set (normalized scale)\n","plt.figure(figsize=(10,5))\n","plt.plot(y_test, label='Actual (Normalized)', color='black')\n","plt.plot(y_pred_test, label='Predicted (Normalized)', linestyle='--', color='blue')\n","plt.title('Normalized Flow - Actual vs. Predicted')\n","plt.xlabel('Sample Index')\n","plt.ylabel('Normalized Flow')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"taG5bF9B3qOQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Cell 8: Inverse Scale Predictions (Flow in original units)\n","\n","# Reconstruct arrays so that we can apply 'scaler.inverse_transform'\n","# We'll fill other feature columns with 0, only place predictions in the flow column.\n","\n","def inverse_transform_predictions(scaler, y_pred, y_actual, target_index, feature_cols):\n","    dummy_pred = np.zeros((len(y_pred), len(feature_cols)))\n","    dummy_pred[:, target_index] = y_pred\n","    inv_pred = scaler.inverse_transform(dummy_pred)[:, target_index]\n","\n","    dummy_actual = np.zeros((len(y_actual), len(feature_cols)))\n","    dummy_actual[:, target_index] = y_actual\n","    inv_actual = scaler.inverse_transform(dummy_actual)[:, target_index]\n","\n","    return inv_pred, inv_actual\n","\n","inv_pred, inv_actual = inverse_transform_predictions(\n","    scaler,\n","    y_pred_test,\n","    y_test,\n","    target_index,\n","    feature_cols\n",")\n","\n","# Plot actual vs predicted flow in original units\n","plt.figure(figsize=(10,5))\n","plt.plot(inv_actual, label='Actual Flow', color='black')\n","plt.plot(inv_pred, label='Predicted Flow', color='blue', linestyle='--')\n","plt.title('Actual vs. Predicted Flow (Original Units)')\n","plt.xlabel('Sample Index')\n","plt.ylabel('Flow (Original Units)')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"qnzM2DL23uHC"},"execution_count":null,"outputs":[]}]}